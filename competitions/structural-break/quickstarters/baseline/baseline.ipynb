{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvWIItAe-0fN"
      },
      "source": [
        "[![Open In Colab](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/documentation/badge/open-in-colab.svg)](https://colab.research.google.com/github/crunchdao/quickstarters/blob/master/competitions/structural-break/quickstarters/baseline/baseline.ipynb)\n",
        "[![Open In Kaggle](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/documentation/badge/open-in-kaggle.svg)](https://www.kaggle.com/code/crunchdao/structural-break-baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DUeixiC_IJM",
        "outputId": "28dd0ec7-aab1-4f46-8128-337af00d7db0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "crunch-cli, version 7.4.0\n",
            "main.py: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/submissions/23863/main.py (6729 bytes)\n",
            "requirements.txt: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/submissions/23863/requirements.original.txt (194 bytes)\n",
            "resources/cnnlstm_model.py: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/models/25094/cnnlstm_model.py (19374 bytes)\n",
            "resources/lgbm_model.py: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/models/25094/lgbm_model.py (17887 bytes)\n",
            "resources/lightgbm_features_cache.pkl: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/models/25094/lightgbm_features_cache.pkl (5784251 bytes)\n",
            "resources/tft_model.py: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/models/25094/tft_model.py (20804 bytes)\n",
            "data/X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\n",
            "data/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\n",
            "data/y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\n",
            "data/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n",
            "                                \n",
            "---\n",
            "Success! Your environment has been correctly setup.\n",
            "Next recommended actions:\n",
            "1. Load the Crunch Toolings: `crunch = crunch.load_notebook()`\n",
            "2. Execute the cells with your code\n",
            "3. Run a test: `crunch.test()`\n",
            "4. Download and submit your code to the platform!\n"
          ]
        }
      ],
      "source": [
        "%pip install crunch-cli --upgrade --quiet --progress-bar off\n",
        "!crunch setup-notebook structural-break MOc241YzAb1h8V4n463c9UXp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-18T09:52:21.302334Z",
          "start_time": "2024-11-18T09:52:18.268241Z"
        },
        "id": "MKqz-6Zw-0fR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import typing\n",
        "\n",
        "# Import your dependencies\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import sklearn.metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjD_WSAS-0fR",
        "outputId": "4e58e59a-808a-4b19-b731-e9d7bf818dee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded inline runner with module: <module '__main__'>\n",
            "\n",
            "cli version: 7.4.0\n",
            "available ram: 12.67 gb\n",
            "available cpu: 2 core\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "import crunch\n",
        "\n",
        "# Load the Crunch Toolings\n",
        "crunch = crunch.load_notebook()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install autocpd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHQWxyiDOWzG",
        "outputId": "232cdab9-3e73-4965-c5d6-ed0540360800"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autocpd\n",
            "  Downloading autocpd-0.1.5-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from autocpd) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from autocpd) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from autocpd) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from autocpd) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from autocpd) (1.16.1)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from autocpd) (0.14.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->autocpd) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->autocpd) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->autocpd) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->autocpd) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->autocpd) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->autocpd) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->autocpd) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->autocpd) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->autocpd) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->autocpd) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->autocpd) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->autocpd) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->autocpd) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->autocpd) (1.17.0)\n",
            "Downloading autocpd-0.1.5-py3-none-any.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m120.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: autocpd\n",
            "Successfully installed autocpd-0.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_pdFvl-Oc8O",
        "outputId": "a5b0d8ae-0f91-4445-97bc-4cf62615a9fc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_docs\n",
            "  Downloading tensorflow_docs-2025.2.19.33219-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting astor (from tensorflow_docs)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from tensorflow_docs) (1.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from tensorflow_docs) (3.1.6)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from tensorflow_docs) (5.10.4)\n",
            "Requirement already satisfied: protobuf>=3.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow_docs) (5.29.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from tensorflow_docs) (6.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->tensorflow_docs) (3.0.2)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->tensorflow_docs) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->tensorflow_docs) (4.25.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from nbformat->tensorflow_docs) (5.8.1)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.11/dist-packages (from nbformat->tensorflow_docs) (5.7.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->tensorflow_docs) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->tensorflow_docs) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->tensorflow_docs) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->tensorflow_docs) (0.26.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->tensorflow_docs) (4.3.8)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat->tensorflow_docs) (4.14.1)\n",
            "Downloading tensorflow_docs-2025.2.19.33219-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.7/182.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Installing collected packages: astor, tensorflow_docs\n",
            "Successfully installed astor-0.8.1 tensorflow_docs-2025.2.19.33219\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_docs.modeling as tfdoc_model\n",
        "import tensorflow_docs.plots as tfdoc_plot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from autocpd.neuralnetwork import compile_and_fit, general_simple_nn,general_deep_nn  # *** Changed to simple_nn ***\n",
        "from autocpd.utils import Transform2D2TR\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "oJlNxj-bOZcL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import tensorflow_docs.modeling as tfdoc_model\n",
        "import tensorflow_docs.plots as tfdoc_plot\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from autocpd.neuralnetwork import compile_and_fit, general_deep_nn\n",
        "from autocpd.utils import DataSetGen, Transform2D2TR"
      ],
      "metadata": {
        "id": "wdr8WQ4ZPg9X"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Time Series Data Loading and Iteration Utilities\n",
        "\n",
        "This module provides utilities for loading and iterating through time series data\n",
        "with boundary point detection and cropping functionality.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Iterator, Tuple, Optional, Dict, Any\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "class TimeSeriesDataLoader:\n",
        "    \"\"\"\n",
        "    A utility class for loading and iterating through time series data.\n",
        "\n",
        "    This class handles:\n",
        "    - Loading multiindex time series data from parquet files\n",
        "    - Iterating through individual time series\n",
        "    - Boundary point detection (where 'period' changes from 0 to 1)\n",
        "    - Cropping time series around boundary points\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 x_path: str = \"data/X_train.parquet\",\n",
        "                 y_path: str = \"data/y_train.parquet\",\n",
        "                 window_size: int = 249,\n",
        "                 keep_period: bool = True):\n",
        "        \"\"\"\n",
        "        Initialize the TimeSeriesDataLoader.\n",
        "\n",
        "        Args:\n",
        "            x_path: Path to the X data parquet file\n",
        "            y_path: Path to the y data parquet file\n",
        "            window_size: Number of timesteps before and after boundary point to include\n",
        "        \"\"\"\n",
        "        self.x_path = x_path\n",
        "        self.y_path = y_path\n",
        "        self.window_size = window_size\n",
        "        self.keep_period = keep_period\n",
        "        # Load data\n",
        "        self._load_data()\n",
        "\n",
        "    def _load_data(self):\n",
        "        \"\"\"Load the data from parquet files.\"\"\"\n",
        "        print(f\"Loading X data from {self.x_path}...\")\n",
        "        self.X = pd.read_parquet(self.x_path)\n",
        "\n",
        "        print(f\"Loading y data from {self.y_path}...\")\n",
        "        self.y = pd.read_parquet(self.y_path)\n",
        "\n",
        "        # Ensure X has proper MultiIndex\n",
        "        if not isinstance(self.X.index, pd.MultiIndex):\n",
        "            if 'id' in self.X.columns and 'time' in self.X.columns:\n",
        "                self.X.set_index(['id', 'time'], inplace=True)\n",
        "            else:\n",
        "                # Try to infer from index structure\n",
        "                if hasattr(self.X.index, 'names') and len(self.X.index.names) == 2:\n",
        "                    self.X.index.names = ['id', 'time']\n",
        "                else:\n",
        "                    raise ValueError(\"X data must have MultiIndex with 'id' and 'time' levels\")\n",
        "\n",
        "        # Process y data\n",
        "        if \"structural_breakpoint\" in self.y.columns:\n",
        "            self.y = self.y[\"structural_breakpoint\"]\n",
        "        else:\n",
        "            self.y = self.y.squeeze()\n",
        "\n",
        "        # Get unique series IDs\n",
        "        self.series_ids = self.X.index.get_level_values('id').unique()\n",
        "        print(f\"Loaded {len(self.series_ids)} time series\")\n",
        "\n",
        "    def get_series_iterator(self,\n",
        "                          crop_around_boundary: bool = False,\n",
        "                          include_y: bool = True) -> Iterator[Tuple[str, pd.DataFrame, Optional[float]]]:\n",
        "        \"\"\"\n",
        "        Get an iterator that yields individual time series.\n",
        "\n",
        "        Args:\n",
        "            crop_around_boundary: If True, crop each series around its boundary point\n",
        "            include_y: If True, include the corresponding y value\n",
        "\n",
        "        Yields:\n",
        "            Tuple of (series_id, series_data, y_value)\n",
        "        \"\"\"\n",
        "        for series_id in self.series_ids:\n",
        "            series_data = self.X.loc[series_id].copy()\n",
        "\n",
        "            if crop_around_boundary:\n",
        "                series_data = self._crop_around_boundary(series_id, series_data)\n",
        "                if series_data is None:\n",
        "                    continue  # Skip series that couldn't be cropped\n",
        "\n",
        "            y_value = self.y.loc[series_id] if include_y else None\n",
        "            if self.keep_period == False:\n",
        "                series_data = series_data.drop(\"period\",axis=1)\n",
        "            yield series_id, series_data, y_value\n",
        "\n",
        "    def _find_boundary_point(self, series_data: pd.DataFrame) -> Optional[int]:\n",
        "        \"\"\"\n",
        "        Find the boundary point in a time series.\n",
        "\n",
        "        Args:\n",
        "            series_data: DataFrame containing the time series data\n",
        "\n",
        "        Returns:\n",
        "            Index of the boundary point, or None if not found\n",
        "        \"\"\"\n",
        "        if 'period' not in series_data.columns:\n",
        "            return None\n",
        "\n",
        "        period = series_data['period'].values\n",
        "\n",
        "        # Find the first index where period goes from 0 to 1\n",
        "        bp_candidates = np.where((period[:-1] == 0) & (period[1:] == 1))[0]\n",
        "\n",
        "        if len(bp_candidates) == 0:\n",
        "            # Fallback: just where period == 1\n",
        "            bp_candidates = np.where(period == 1)[0]\n",
        "\n",
        "        if len(bp_candidates) == 0:\n",
        "            return None\n",
        "\n",
        "        # Return the first boundary point\n",
        "        bp = bp_candidates[0] + 1 if (period[:-1] == 0).any() else bp_candidates[0]\n",
        "        return bp\n",
        "\n",
        "    def _crop_around_boundary(self, series_id: str, series_data: pd.DataFrame) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Crop a time series around its boundary point.\n",
        "\n",
        "        Args:\n",
        "            series_id: ID of the time series\n",
        "            series_data: DataFrame containing the time series data\n",
        "\n",
        "        Returns:\n",
        "            Cropped DataFrame or None if cropping is not possible\n",
        "        \"\"\"\n",
        "        bp = self._find_boundary_point(series_data)\n",
        "\n",
        "        if bp is None:\n",
        "            print(f\"Warning: No boundary point found for series {series_id}\")\n",
        "            return None\n",
        "\n",
        "        start = bp - self.window_size\n",
        "        end = bp + self.window_size-1\n",
        "\n",
        "        if start < 0 or end >= len(series_data):\n",
        "            print(f\"Warning: Series {series_id} too short for cropping (length: {len(series_data)}, required: {2*self.window_size + 1})\")\n",
        "            return None\n",
        "\n",
        "        cropped_data = series_data.iloc[start:end+1].copy()\n",
        "\n",
        "        # Reset the time index to start from 1\n",
        "        cropped_data.index = pd.RangeIndex(1, len(cropped_data) + 1)\n",
        "\n",
        "        return cropped_data\n",
        "\n",
        "    def get_series_by_id(self, series_id: str, crop_around_boundary: bool = False) -> Tuple[pd.DataFrame, float]:\n",
        "        \"\"\"\n",
        "        Get a specific time series by ID.\n",
        "\n",
        "        Args:\n",
        "            series_id: ID of the time series to retrieve\n",
        "            crop_around_boundary: If True, crop around boundary point\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (series_data, y_value)\n",
        "        \"\"\"\n",
        "        if series_id not in self.series_ids:\n",
        "            raise ValueError(f\"Series ID {series_id} not found in dataset\")\n",
        "\n",
        "        series_data = self.X.loc[series_id].copy()\n",
        "        if crop_around_boundary:\n",
        "            boundary_point = self.window_size +1\n",
        "        else:\n",
        "            boundary_point = self._find_boundary_point(series_data)\n",
        "        if crop_around_boundary:\n",
        "            series_data = self._crop_around_boundary(series_id, series_data)\n",
        "            if series_data is None:\n",
        "                raise ValueError(f\"Could not crop series {series_id} around boundary point\")\n",
        "        if self.keep_period == False:\n",
        "                series_data = series_data.drop(\"period\",axis=1)\n",
        "        y_value = self.y.loc[series_id]\n",
        "        return series_data, y_value, boundary_point\n",
        "\n",
        "    def get_all_cropped_series(self) -> Tuple[pd.DataFrame, pd.Series]:\n",
        "        \"\"\"\n",
        "        Get all time series cropped around their boundary points.\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (X_cropped, y_cropped) where X_cropped is a MultiIndex DataFrame\n",
        "        \"\"\"\n",
        "        cropped_series = []\n",
        "        kept_ids = []\n",
        "\n",
        "        for series_id, series_data, y_value in self.get_series_iterator(crop_around_boundary=True):\n",
        "            cropped_data = self._crop_around_boundary(series_id, series_data)\n",
        "            if cropped_data is not None:\n",
        "                # Set proper MultiIndex for the cropped data\n",
        "                cropped_data.index = pd.MultiIndex.from_product(\n",
        "                    [[series_id], range(1, len(cropped_data) + 1)],\n",
        "                    names=['id', 'time']\n",
        "                )\n",
        "                cropped_series.append(cropped_data)\n",
        "                kept_ids.append(series_id)\n",
        "\n",
        "        if not cropped_series:\n",
        "            raise ValueError(\"No series could be cropped successfully\")\n",
        "\n",
        "        X_cropped = pd.concat(cropped_series)\n",
        "        y_cropped = self.y.loc[kept_ids]\n",
        "        if self.keep_period == False:\n",
        "                X_cropped = X_cropped.drop(\"period\",axis=1)\n",
        "        print(f\"Successfully cropped {len(kept_ids)} out of {len(self.series_ids)} series\")\n",
        "\n",
        "        return X_cropped, y_cropped\n",
        "\n",
        "    def get_series_info(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Get information about the loaded dataset.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with dataset information\n",
        "        \"\"\"\n",
        "        info = {\n",
        "            'total_series': len(self.series_ids),\n",
        "            'columns': list(self.X.columns),\n",
        "            'total_timesteps': len(self.X),\n",
        "            'avg_series_length': len(self.X) / len(self.series_ids),\n",
        "            'window_size': self.window_size,\n",
        "            'y_unique_values': self.y.unique().tolist(),\n",
        "            'y_distribution': self.y.value_counts().to_dict()\n",
        "        }\n",
        "        return info\n",
        "\n",
        "    def save_cropped_data(self,\n",
        "                         x_output_path: str = \"X_train_cropped.parquet\",\n",
        "                         y_output_path: str = \"y_train_cropped.parquet\"):\n",
        "        \"\"\"\n",
        "        Save all cropped time series to parquet files.\n",
        "\n",
        "        Args:\n",
        "            x_output_path: Path to save cropped X data\n",
        "            y_output_path: Path to save cropped y data\n",
        "        \"\"\"\n",
        "        X_cropped, y_cropped = self.get_all_cropped_series()\n",
        "\n",
        "        X_cropped.to_parquet(x_output_path)\n",
        "        y_cropped.to_frame().to_parquet(y_output_path)\n",
        "\n",
        "        print(f\"Saved cropped data to {x_output_path} and {y_output_path}\")\n",
        "\n",
        "    def get_series_before_after_by_id(self, series_id: str, crop_around_boundary: bool = False) -> Tuple[pd.DataFrame, pd.DataFrame, float, int]:\n",
        "        \"\"\"\n",
        "        Get a specific time series by ID, returning two series: before and after the boundary point.\n",
        "\n",
        "        Args:\n",
        "            series_id: ID of the time series to retrieve\n",
        "            crop_around_boundary: If True, crop to window_size timesteps before and after boundary point\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (series_before, series_after, y_value, boundary_point_index)\n",
        "        \"\"\"\n",
        "        if series_id not in self.series_ids:\n",
        "            raise ValueError(f\"Series ID {series_id} not found in dataset\")\n",
        "\n",
        "        series_data = self.X.loc[series_id].copy()\n",
        "        boundary_point = self._find_boundary_point(series_data)\n",
        "\n",
        "        if boundary_point is None:\n",
        "            raise ValueError(f\"No boundary point found for series {series_id}\")\n",
        "\n",
        "        if crop_around_boundary:\n",
        "            # Crop to window_size before and after boundary point\n",
        "            start_before = max(0, boundary_point - self.window_size)\n",
        "            end_before = boundary_point\n",
        "            start_after = boundary_point\n",
        "            end_after = min(len(series_data), boundary_point + self.window_size)\n",
        "        else:\n",
        "            # Use full series before and after boundary point\n",
        "            start_before = 0\n",
        "            end_before = boundary_point\n",
        "            start_after = boundary_point\n",
        "            end_after = len(series_data)\n",
        "\n",
        "        # Extract before and after series\n",
        "        series_before = series_data.iloc[start_before:end_before].copy()\n",
        "        series_after = series_data.iloc[start_after:end_after].copy()\n",
        "\n",
        "        # Reset indices to start from 1\n",
        "        series_before.index = pd.RangeIndex(1, len(series_before) + 1)\n",
        "        series_after.index = pd.RangeIndex(1, len(series_after) + 1)\n",
        "\n",
        "        # Remove period column if not keeping it\n",
        "        if self.keep_period == False:\n",
        "            series_before = series_before.drop(\"period\", axis=1)\n",
        "            series_after = series_after.drop(\"period\", axis=1)\n",
        "\n",
        "        y_value = self.y.loc[series_id]\n",
        "        return series_before, series_after, y_value, boundary_point\n",
        "\n",
        "\n",
        "# Convenience functions for quick access\n",
        "def load_timeseries_data(x_path: str = \"data/X_train.parquet\",\n",
        "                        y_path: str = \"data/y_train.parquet\") -> TimeSeriesDataLoader:\n",
        "    \"\"\"\n",
        "    Convenience function to create a TimeSeriesDataLoader instance.\n",
        "\n",
        "    Args:\n",
        "        x_path: Path to X data\n",
        "        y_path: Path to y data\n",
        "\n",
        "    Returns:\n",
        "        TimeSeriesDataLoader instance\n",
        "    \"\"\"\n",
        "    return TimeSeriesDataLoader(x_path, y_path)\n",
        "\n",
        "\n",
        "def iterate_series(loader: TimeSeriesDataLoader,\n",
        "                  crop_around_boundary: bool = False) -> Iterator[Tuple[str, pd.DataFrame, float]]:\n",
        "    \"\"\"\n",
        "    Convenience function to iterate through all series.\n",
        "\n",
        "    Args:\n",
        "        loader: TimeSeriesDataLoader instance\n",
        "        crop_around_boundary: Whether to crop around boundary points\n",
        "\n",
        "    Yields:\n",
        "        Tuple of (series_id, series_data, y_value)\n",
        "    \"\"\"\n",
        "    for series_id, series_data, y_value in loader.get_series_iterator(crop_around_boundary=crop_around_boundary):\n",
        "        yield series_id, series_data, y_value\n",
        "\n"
      ],
      "metadata": {
        "id": "zcrh6cRAOl8W"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_series = []\n",
        "all_labels = []\n",
        "for i in range (0,10000,1):\n",
        "  series_data,y_value,boundary = loader.get_series_by_id(i,True)\n",
        "  series_data_clean = series_data[\"value\"]\n",
        "  all_series.append(series_data_clean)\n",
        "  all_labels.append(y_value)\n",
        "data_x = np.array(all_series)  # Shape: (num_ids, 498)\n",
        "labels = np.array(all_labels)   # Shape: (num_ids,)\n",
        "\n",
        "print(f\"Data shape: {data_x.shape}\")\n",
        "print(f\"Labels shape: {labels.shape}\")\n",
        "\n",
        "true_indices = np.where(labels == True)[0]\n",
        "false_indices = np.where(labels == False)[0]\n",
        "\n",
        "print(f\"True samples: {len(true_indices)}\")   # ~3333\n",
        "print(f\"False samples: {len(false_indices)}\") # ~6666\n",
        "\n",
        "from sklearn.utils import resample\n",
        "\n",
        "false_balanced = resample(false_indices,\n",
        "                         replace=False,\n",
        "                         n_samples=len(true_indices),\n",
        "                         random_state=42)\n",
        "\n",
        "# Combine balanced indices\n",
        "balanced_indices = np.concatenate([true_indices, false_balanced])\n",
        "np.random.shuffle(balanced_indices)\n",
        "\n",
        "# Create balanced dataset by indexing your existing arrays\n",
        "data_x_balanced = data_x[balanced_indices]\n",
        "labels_balanced = labels[balanced_indices].astype(int)  # Convert True/False to 1/0\n",
        "\n",
        "print(f\"Balanced data shape: {data_x_balanced.shape}\")  # Should be (~6666, 498)\n",
        "print(f\"Balanced labels shape: {labels_balanced.shape}\")\n",
        "print(f\"Balanced class distribution: {np.bincount(labels_balanced)}\")\n",
        "\n",
        "# Now use data_x_balanced and labels_balanced for your model\n",
        "data_x = data_x_balanced\n",
        "labels = labels_balanced\n",
        "\n",
        "# Method 1: Per-sample min-max normalization (RECOMMENDED)\n",
        "# This preserves the shape of each time series while normalizing to [0,1]\n",
        "def normalize_per_sample(data):\n",
        "    normalized = np.zeros_like(data)\n",
        "    for i in range(data.shape[0]):\n",
        "        sample = data[i]\n",
        "        sample_min = sample.min()\n",
        "        sample_max = sample.max()\n",
        "        if sample_max > sample_min:  # Avoid division by zero\n",
        "            normalized[i] = (sample - sample_min) / (sample_max - sample_min)\n",
        "        else:\n",
        "            normalized[i] = np.zeros_like(sample)  # Flat signal = all zeros\n",
        "    return normalized\n",
        "\n",
        "# Apply the better normalization\n",
        "data_x_norm = normalize_per_sample(data_x_balanced)\n",
        "\n",
        "# Check if this fixes the variation issue\n",
        "print(\"After per-sample normalization:\")\n",
        "print(f\"First sample std: {data_x_norm[0].std():.4f}\")\n",
        "print(f\"Sample means: {data_x_norm.mean(axis=1)[:5]}\")\n",
        "print(f\"Sample stds: {data_x_norm.std(axis=1)[:5]}\")\n",
        "print(f\"Overall range: [{data_x_norm.min():.4f}, {data_x_norm.max():.4f}]\")\n",
        "\n",
        "# Split and train\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    data_x_norm, labels_balanced.reshape(-1, 1), train_size=0.8, random_state=42\n",
        ")\n",
        "\n",
        "x_train = Transform2D2TR(x_train, rescale=False, times=3)\n",
        "x_test = Transform2D2TR(x_test, rescale=False, times=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZdHtEYiO04a",
        "outputId": "24e9c3bd-05b3-4047-952f-b9e1bf5145d5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: (10000, 498)\n",
            "Labels shape: (10000,)\n",
            "True samples: 2908\n",
            "False samples: 7092\n",
            "Balanced data shape: (5816, 498)\n",
            "Balanced labels shape: (5816,)\n",
            "Balanced class distribution: [2908 2908]\n",
            "After per-sample normalization:\n",
            "First sample std: 0.1193\n",
            "Sample means: [0.64402393 0.44044606 0.42643469 0.44594874 0.39690033]\n",
            "Sample stds: [0.11928432 0.18106374 0.11487443 0.06359554 0.08363304]\n",
            "Overall range: [0.0000, 1.0000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "epochs = 100\n",
        "batch_size = 64\n",
        "dropout_rate = 0.3\n",
        "n_filter = 16\n",
        "n = x_train.shape[-1]\n",
        "num_tran = x_train.shape[1]\n",
        "kernel_size = (num_tran // 2, 10)\n",
        "num_classes = 2\n",
        "logdir = Path(\"tensorboard_logs\", \"Trial\")\n",
        "# %%\n",
        "num_resblock = 3\n",
        "model_name = \"Toshko\"\n",
        "print(model_name)\n",
        "# build the model\n",
        "m = np.array([50, 40, 30, 20, 10])\n",
        "model = general_deep_nn(\n",
        "    n,\n",
        "    num_tran,\n",
        "    kernel_size,\n",
        "    n_filter,\n",
        "    dropout_rate,\n",
        "    num_classes,\n",
        "    num_resblock,\n",
        "    m,\n",
        "    5,\n",
        "    model_name=model_name,\n",
        ")\n",
        "model.summary()\n",
        "\n",
        "size_histories = {}\n",
        "epochdots = tfdoc_model.EpochDots()\n",
        "size_histories[model_name] = compile_and_fit(\n",
        "    model,\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size,\n",
        "    learning_rate,\n",
        "    model_name,\n",
        "    logdir,\n",
        "    epochdots,\n",
        "    validation_split=0.25,\n",
        "    max_epochs=epochs,\n",
        ")\n",
        "plotter = tfdoc_plot.HistoryPlotter(metric=\"accuracy\", smoothing_std=10)\n",
        "plt.figure(figsize=(10, 8))\n",
        "plotter.plot(size_histories)\n",
        "acc_name = model_name + \"+acc.png\"\n",
        "acc_path = Path(logdir, model_name, acc_name)\n",
        "plt.savefig(acc_path)\n",
        "\n",
        "\n",
        "model_path = Path(logdir, model_name, \"model\")\n",
        "model.save(model_path)\n",
        "\n",
        "#  Confusion Matrix\n",
        "model_pred = model.evaluate(x_test, y_test, verbose=2)\n",
        "y_prob = np.max(model.predict(x_test), axis=1)\n",
        "y_pred = np.argmax(model.predict(x_test), axis=1)\n",
        "confusion_mtx = tf.math.confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "label_vec = np.arange(num_classes)\n",
        "sns.heatmap(\n",
        "    confusion_mtx,\n",
        "    cmap=\"YlGnBu\",\n",
        "    xticklabels=label_vec,\n",
        "    yticklabels=label_vec,\n",
        "    annot=True,\n",
        "    fmt=\"g\",\n",
        ")\n",
        "plt.xlabel(\"Prediction\")\n",
        "plt.ylabel(\"Label\")\n",
        "cm_name = model_name + \"Confusion_matrix.png\"\n",
        "cm_path = Path(logdir, model_name, cm_name)\n",
        "plt.savefig(cm_path)\n",
        "\n",
        "# save the confusion matrix\n",
        "path_confusion_matrix = Path(logdir, model_name, \"confusion_matrix\")\n",
        "np.save(path_confusion_matrix, confusion_mtx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_U2rEQkYPQly",
        "outputId": "4ac65389-61dd-417c-8929-0a07e6e33787"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Toshko\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"Toshko\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Toshko\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ Input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m498\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape (\u001b[38;5;33mReshape\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m498\u001b[0m, \u001b[38;5;34m1\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ Input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m498\u001b[0m,    │         \u001b[38;5;34m80\u001b[0m │ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m498\u001b[0m,    │         \u001b[38;5;34m64\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu (\u001b[38;5;33mReLU\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m498\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ re_lu[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │      \u001b[38;5;34m7,696\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │         \u001b[38;5;34m64\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_1 (\u001b[38;5;33mReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │      \u001b[38;5;34m7,696\u001b[0m │ re_lu_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │         \u001b[38;5;34m64\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_2 (\u001b[38;5;33mReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │      \u001b[38;5;34m7,696\u001b[0m │ re_lu_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │         \u001b[38;5;34m64\u001b[0m │ conv2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_3 (\u001b[38;5;33mReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │      \u001b[38;5;34m7,696\u001b[0m │ re_lu_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │         \u001b[38;5;34m64\u001b[0m │ conv2d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ re_lu_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_4 (\u001b[38;5;33mReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │      \u001b[38;5;34m7,696\u001b[0m │ re_lu_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │         \u001b[38;5;34m64\u001b[0m │ conv2d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_5 (\u001b[38;5;33mReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │      \u001b[38;5;34m7,696\u001b[0m │ re_lu_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │         \u001b[38;5;34m64\u001b[0m │ conv2d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_2 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ re_lu_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_6 (\u001b[38;5;33mReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ re_lu_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │        \u001b[38;5;34m850\u001b[0m │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)        │      \u001b[38;5;34m2,040\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)        │      \u001b[38;5;34m1,230\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │        \u001b[38;5;34m620\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │        \u001b[38;5;34m210\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │         \u001b[38;5;34m22\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ Input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">498</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">498</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ Input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">498</span>,    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">498</span>,    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">498</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ re_lu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,696</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,696</span> │ re_lu_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,696</span> │ re_lu_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ conv2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,696</span> │ re_lu_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ conv2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ re_lu_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,696</span> │ re_lu_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ conv2d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,696</span> │ re_lu_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ conv2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ re_lu_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ re_lu_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">850</span> │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,040</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,230</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">620</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">210</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m51,676\u001b[0m (201.86 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">51,676</span> (201.86 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m51,452\u001b[0m (200.98 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">51,452</span> (200.98 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m224\u001b[0m (896.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">224</span> (896.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\n",
            "Epoch: 0, accuracy:0.4918,  loss:1.8557,  sparse_categorical_crossentropy:0.7044,  val_accuracy:0.4824,  val_loss:1.6157,  val_sparse_categorical_crossentropy:0.6933,  \n",
            ".55/55 - 27s - 485ms/step - accuracy: 0.4918 - loss: 1.8557 - sparse_categorical_crossentropy: 0.7044 - val_accuracy: 0.4824 - val_loss: 1.6157 - val_sparse_categorical_crossentropy: 0.6933\n",
            "Epoch 2/100\n",
            ".55/55 - 22s - 396ms/step - accuracy: 0.5062 - loss: 1.4477 - sparse_categorical_crossentropy: 0.6934 - val_accuracy: 0.5159 - val_loss: 1.2921 - val_sparse_categorical_crossentropy: 0.6931\n",
            "Epoch 3/100\n",
            ".55/55 - 2s - 36ms/step - accuracy: 0.5010 - loss: 1.1802 - sparse_categorical_crossentropy: 0.6936 - val_accuracy: 0.5176 - val_loss: 1.0762 - val_sparse_categorical_crossentropy: 0.6928\n",
            "Epoch 4/100\n",
            ".55/55 - 2s - 35ms/step - accuracy: 0.4990 - loss: 1.0029 - sparse_categorical_crossentropy: 0.6930 - val_accuracy: 0.5176 - val_loss: 0.9359 - val_sparse_categorical_crossentropy: 0.6929\n",
            "Epoch 5/100\n",
            ".55/55 - 2s - 35ms/step - accuracy: 0.5053 - loss: 0.8891 - sparse_categorical_crossentropy: 0.6932 - val_accuracy: 0.5176 - val_loss: 0.8462 - val_sparse_categorical_crossentropy: 0.6929\n",
            "Epoch 6/100\n",
            ".55/55 - 3s - 48ms/step - accuracy: 0.5016 - loss: 0.8167 - sparse_categorical_crossentropy: 0.6932 - val_accuracy: 0.5176 - val_loss: 0.7893 - val_sparse_categorical_crossentropy: 0.6928\n",
            "Epoch 7/100\n",
            ".55/55 - 2s - 36ms/step - accuracy: 0.4821 - loss: 0.7712 - sparse_categorical_crossentropy: 0.6934 - val_accuracy: 0.5176 - val_loss: 0.7540 - val_sparse_categorical_crossentropy: 0.6931\n",
            "Epoch 8/100\n",
            ".55/55 - 2s - 45ms/step - accuracy: 0.5030 - loss: 0.7424 - sparse_categorical_crossentropy: 0.6932 - val_accuracy: 0.5176 - val_loss: 0.7316 - val_sparse_categorical_crossentropy: 0.6929\n",
            "Epoch 9/100\n",
            ".55/55 - 2s - 35ms/step - accuracy: 0.5030 - loss: 0.7245 - sparse_categorical_crossentropy: 0.6932 - val_accuracy: 0.5176 - val_loss: 0.7178 - val_sparse_categorical_crossentropy: 0.6930\n",
            "Epoch 10/100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-821029814.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0msize_histories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mepochdots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfdoc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEpochDots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m size_histories[model_name] = compile_and_fit(\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autocpd/neuralnetwork.py\u001b[0m in \u001b[0;36mcompile_and_fit\u001b[0;34m(model, x_train, y_train, batch_size, lr, name, log_dir, epochdots, optimizer, validation_split, max_epochs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         ],\n\u001b[1;32m    165\u001b[0m     )\n\u001b[0;32m--> 166\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[1;32m    220\u001b[0m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/optional_ops.py\u001b[0m in \u001b[0;36mhas_value\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    174\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m       return gen_optional_ops.optional_has_value(\n\u001b[0m\u001b[1;32m    177\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_optional_ops.py\u001b[0m in \u001b[0;36moptional_has_value\u001b[0;34m(optional, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m    173\u001b[0m         _ctx, \"OptionalHasValue\", name, optional)\n\u001b[1;32m    174\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "wrfJmLF-Rb5E"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_deepnn_for_window(window_size=249):\n",
        "  all_series = []\n",
        "  all_labels = []\n",
        "  loader = TimeSeriesDataLoader(window_size=window_size)\n",
        "  for i in range (0,10000,1):\n",
        "    series_data,y_value,boundary = loader.get_series_by_id(i,True)\n",
        "    series_data_clean = series_data[\"value\"]\n",
        "    series_data_clean = series_data_clean[boundary-10:] ## trimmt so, dass der Anfang 10 Timesteps vor dem boundary point ist\n",
        "    all_series.append(series_data_clean)\n",
        "    all_labels.append(y_value)\n",
        "  data_x = np.array(all_series)  # Shape: (num_ids, 498)\n",
        "  labels = np.array(all_labels)   # Shape: (num_ids,)\n",
        "\n",
        "  print(f\"Data shape: {data_x.shape}\")\n",
        "  print(f\"Labels shape: {labels.shape}\")\n",
        "\n",
        "  true_indices = np.where(labels == True)[0]\n",
        "  false_indices = np.where(labels == False)[0]\n",
        "\n",
        "  print(f\"True samples: {len(true_indices)}\")   # ~3333\n",
        "  print(f\"False samples: {len(false_indices)}\") # ~6666\n",
        "\n",
        "  from sklearn.utils import resample\n",
        "\n",
        "  false_balanced = resample(false_indices,\n",
        "                          replace=False,\n",
        "                          n_samples=len(true_indices),\n",
        "                          random_state=42)\n",
        "\n",
        "  # Combine balanced indices\n",
        "  balanced_indices = np.concatenate([true_indices, false_balanced])\n",
        "  np.random.shuffle(balanced_indices)\n",
        "\n",
        "  # Create balanced dataset by indexing your existing arrays\n",
        "  data_x_balanced = data_x[balanced_indices]\n",
        "  labels_balanced = labels[balanced_indices].astype(int)  # Convert True/False to 1/0\n",
        "\n",
        "  print(f\"Balanced data shape: {data_x_balanced.shape}\")  # Should be (~6666, 498)\n",
        "  print(f\"Balanced labels shape: {labels_balanced.shape}\")\n",
        "  print(f\"Balanced class distribution: {np.bincount(labels_balanced)}\")\n",
        "\n",
        "  # Now use data_x_balanced and labels_balanced for your model\n",
        "  data_x = data_x_balanced\n",
        "  labels = labels_balanced\n",
        "\n",
        "  # Method 1: Per-sample min-max normalization (RECOMMENDED)\n",
        "  # This preserves the shape of each time series while normalizing to [0,1]\n",
        "  def normalize_per_sample(data):\n",
        "      normalized = np.zeros_like(data)\n",
        "      for i in range(data.shape[0]):\n",
        "          sample = data[i]\n",
        "          sample_min = sample.min()\n",
        "          sample_max = sample.max()\n",
        "          if sample_max > sample_min:  # Avoid division by zero\n",
        "              normalized[i] = (sample - sample_min) / (sample_max - sample_min)\n",
        "          else:\n",
        "              normalized[i] = np.zeros_like(sample)  # Flat signal = all zeros\n",
        "      return normalized\n",
        "\n",
        "  # Apply the better normalization\n",
        "  data_x_norm = normalize_per_sample(data_x_balanced)\n",
        "\n",
        "  # Check if this fixes the variation issue\n",
        "  print(\"After per-sample normalization:\")\n",
        "  print(f\"First sample std: {data_x_norm[0].std():.4f}\")\n",
        "  print(f\"Sample means: {data_x_norm.mean(axis=1)[:5]}\")\n",
        "  print(f\"Sample stds: {data_x_norm.std(axis=1)[:5]}\")\n",
        "  print(f\"Overall range: [{data_x_norm.min():.4f}, {data_x_norm.max():.4f}]\")\n",
        "\n",
        "  # Split and train\n",
        "  x_train, x_test, y_train, y_test = train_test_split(\n",
        "      data_x_norm, labels_balanced.reshape(-1, 1), train_size=0.8, random_state=42\n",
        "  )\n",
        "\n",
        "  x_train = Transform2D2TR(x_train, rescale=False, times=3)\n",
        "  x_test = Transform2D2TR(x_test, rescale=False, times=3)\n",
        "\n",
        "\"\"\"\n",
        "  learning_rate = 5e-5\n",
        "  epochs = 300\n",
        "  batch_size = 64\n",
        "  dropout_rate = 0.3\n",
        "  n_filter = 16\n",
        "  n = x_train.shape[-1]\n",
        "  num_tran = x_train.shape[1]\n",
        "  kernel_size = (num_tran // 2, 10)\n",
        "  num_classes = 2\n",
        "  logdir = Path(\"tensorboard_logs\", \"Trial\")\n",
        "  # %%\n",
        "  num_resblock = 3\n",
        "  model_name = \"Toshko\"\n",
        "  print(model_name)\n",
        "  # build the model\n",
        "  m = np.array([50, 40, 30, 20, 10])\n",
        "  model = general_deep_nn(\n",
        "      n,\n",
        "      num_tran,\n",
        "      kernel_size,\n",
        "      n_filter,\n",
        "      dropout_rate,\n",
        "      num_classes,\n",
        "      num_resblock,\n",
        "      m,\n",
        "      5,\n",
        "      model_name=model_name,\n",
        "  )\n",
        "  model.summary()\n",
        "\n",
        "  size_histories = {}\n",
        "  epochdots = tfdoc_model.EpochDots()\n",
        "  size_histories[model_name] = compile_and_fit(\n",
        "      model,\n",
        "      x_train,\n",
        "      y_train,\n",
        "      batch_size,\n",
        "      learning_rate,\n",
        "      model_name,\n",
        "      logdir,\n",
        "      epochdots,\n",
        "      validation_split=0.25,\n",
        "      max_epochs=epochs,\n",
        "  )\n",
        "  plotter = tfdoc_plot.HistoryPlotter(metric=\"accuracy\", smoothing_std=10)\n",
        "  plt.figure(figsize=(10, 8))\n",
        "  plotter.plot(size_histories)\n",
        "  acc_name = model_name + \"+acc.png\"\n",
        "  acc_path = Path(logdir, model_name, acc_name)\n",
        "  plt.savefig(acc_path)\n",
        "\n",
        "\n",
        "  model_path = Path(logdir, model_name, \"model\")\n",
        " # model.save(model_path)\n",
        "\n",
        "  #  Confusion Matrix\n",
        "  model_pred = model.evaluate(x_test, y_test, verbose=2)\n",
        "  y_prob = np.max(model.predict(x_test), axis=1)\n",
        "  y_pred = np.argmax(model.predict(x_test), axis=1)\n",
        "  confusion_mtx = tf.math.confusion_matrix(y_test, y_pred)\n",
        "  plt.figure(figsize=(10, 8))\n",
        "  label_vec = np.arange(num_classes)\n",
        "  sns.heatmap(\n",
        "      confusion_mtx,\n",
        "      cmap=\"YlGnBu\",\n",
        "      xticklabels=label_vec,\n",
        "      yticklabels=label_vec,\n",
        "      annot=True,\n",
        "      fmt=\"g\",\n",
        "  )\n",
        "  plt.xlabel(\"Prediction\")\n",
        "  plt.ylabel(\"Label\")\n",
        "\n",
        "  # Calculate ROC AUC\n",
        "  roc_auc = roc_auc_score(y_test, y_pred)\n",
        "  print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "yHPTD9amQV4L",
        "outputId": "78c1acfd-6dfc-4514-fa1c-98237bddeed8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  learning_rate = 5e-5\\n  epochs = 300\\n  batch_size = 64\\n  dropout_rate = 0.3\\n  n_filter = 16\\n  n = x_train.shape[-1]\\n  num_tran = x_train.shape[1]\\n  kernel_size = (num_tran // 2, 10)\\n  num_classes = 2\\n  logdir = Path(\"tensorboard_logs\", \"Trial\")\\n  # %%\\n  num_resblock = 3\\n  model_name = \"Toshko\"\\n  print(model_name)\\n  # build the model\\n  m = np.array([50, 40, 30, 20, 10])\\n  model = general_deep_nn(\\n      n,\\n      num_tran,\\n      kernel_size,\\n      n_filter,\\n      dropout_rate,\\n      num_classes,\\n      num_resblock,\\n      m,\\n      5,\\n      model_name=model_name,\\n  )\\n  model.summary()\\n\\n  size_histories = {}\\n  epochdots = tfdoc_model.EpochDots()\\n  size_histories[model_name] = compile_and_fit(\\n      model,\\n      x_train,\\n      y_train,\\n      batch_size,\\n      learning_rate,\\n      model_name,\\n      logdir,\\n      epochdots,\\n      validation_split=0.25,\\n      max_epochs=epochs,\\n  )\\n  plotter = tfdoc_plot.HistoryPlotter(metric=\"accuracy\", smoothing_std=10)\\n  plt.figure(figsize=(10, 8))\\n  plotter.plot(size_histories)\\n  acc_name = model_name + \"+acc.png\"\\n  acc_path = Path(logdir, model_name, acc_name)\\n  plt.savefig(acc_path)\\n\\n\\n  model_path = Path(logdir, model_name, \"model\")\\n # model.save(model_path)\\n\\n  #  Confusion Matrix\\n  model_pred = model.evaluate(x_test, y_test, verbose=2)\\n  y_prob = np.max(model.predict(x_test), axis=1)\\n  y_pred = np.argmax(model.predict(x_test), axis=1)\\n  confusion_mtx = tf.math.confusion_matrix(y_test, y_pred)\\n  plt.figure(figsize=(10, 8))\\n  label_vec = np.arange(num_classes)\\n  sns.heatmap(\\n      confusion_mtx,\\n      cmap=\"YlGnBu\",\\n      xticklabels=label_vec,\\n      yticklabels=label_vec,\\n      annot=True,\\n      fmt=\"g\",\\n  )\\n  plt.xlabel(\"Prediction\")\\n  plt.ylabel(\"Label\")\\n\\n  # Calculate ROC AUC\\n  roc_auc = roc_auc_score(y_test, y_pred)\\n  print(f\"ROC AUC Score: {roc_auc:.4f}\")\\n  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for window_size in [10,20,30,50,100]:\n",
        "  test_deepnn_for_window(window_size)"
      ],
      "metadata": {
        "id": "43LiU4ttQy-X",
        "outputId": "3c672165-6af8-4a3f-c380-0da589241460",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 981
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading X data from data/X_train.parquet...\n",
            "Loading y data from data/y_train.parquet...\n",
            "Loaded 10001 time series\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _xla_gc_callback at 0x7e26683763e0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py\", line 96, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "    \n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: (10000, 19)\n",
            "Labels shape: (10000,)\n",
            "True samples: 2908\n",
            "False samples: 7092\n",
            "Balanced data shape: (5816, 19)\n",
            "Balanced labels shape: (5816,)\n",
            "Balanced class distribution: [2908 2908]\n",
            "After per-sample normalization:\n",
            "First sample std: 0.2671\n",
            "Sample means: [0.49843171 0.49099493 0.64992319 0.28860509 0.51416152]\n",
            "Sample stds: [0.2670732  0.27057148 0.21044334 0.19730819 0.25259211]\n",
            "Overall range: [0.0000, 1.0000]\n",
            "Loading X data from data/X_train.parquet...\n",
            "Loading y data from data/y_train.parquet...\n",
            "Loaded 10001 time series\n",
            "Data shape: (10000, 29)\n",
            "Labels shape: (10000,)\n",
            "True samples: 2908\n",
            "False samples: 7092\n",
            "Balanced data shape: (5816, 29)\n",
            "Balanced labels shape: (5816,)\n",
            "Balanced class distribution: [2908 2908]\n",
            "After per-sample normalization:\n",
            "First sample std: 0.2567\n",
            "Sample means: [0.46123719 0.40825061 0.53270136 0.59324478 0.5670513 ]\n",
            "Sample stds: [0.25674367 0.30650708 0.24781146 0.2337478  0.22835682]\n",
            "Overall range: [0.0000, 1.0000]\n",
            "Loading X data from data/X_train.parquet...\n",
            "Loading y data from data/y_train.parquet...\n",
            "Loaded 10001 time series\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1327634827.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwindow_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtest_deepnn_for_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3949803941.py\u001b[0m in \u001b[0;36mtest_deepnn_for_window\u001b[0;34m(window_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mall_series\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_data_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mall_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mdata_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_series\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Shape: (num_ids, 498)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Shape: (num_ids,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6294\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6296\u001b[0;31m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_can_hold_identifiers_and_holds_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5446\u001b[0m         \"\"\"\n\u001b[1;32m   5447\u001b[0m         if (\n\u001b[0;32m-> 5448\u001b[0;31m             \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5449\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0mis_string_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5450\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_object_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \"\"\"\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_is_dtype_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36m_is_dtype_type\u001b[0;34m(arr_or_dtype, condition)\u001b[0m\n\u001b[1;32m   1451\u001b[0m     \u001b[0;31m# fastpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1452\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1453\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1454\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExtensionDtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(tipo)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mklasses\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;34m\"\"\"Evaluate if the tipo is a subclass of the klasses.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mtipo\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtipo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mklasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import layers, models, losses, metrics\n",
        "from sklearn.utils import shuffle\n",
        "import warnings\n",
        "class FocusedChangePointDetector:\n",
        "  \"\"\"\n",
        "  A changepoint detector that focuses on determining if a specific point\n",
        "  in a time series is a changepoint or not.\n",
        "  \"\"\"\n",
        "  def __init__(self, window_size=50, n_transformations=4):\n",
        "      \"\"\"\n",
        "      Parameters\n",
        "      ----------\n",
        "      window_size : int\n",
        "          Size of the window around the target point (total window = 2 * window_size)\n",
        "      n_transformations : int\n",
        "          Number of transformations to apply to the time series\n",
        "      \"\"\"\n",
        "      self.window_size = window_size\n",
        "      self.n_transformations = n_transformations\n",
        "      self.model = None\n",
        "\n",
        "  def create_centered_windows(self, time_series, target_indices, window_size=None):\n",
        "      \"\"\"\n",
        "      Create windows centered around specific target points.\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      time_series : np.ndarray\n",
        "          Shape (n,) or (batch_size, n) - the time series data\n",
        "      target_indices : np.ndarray\n",
        "          Indices of points to check for changepoints\n",
        "      window_size : int\n",
        "          Half-window size (full window will be 2 * window_size)\n",
        "\n",
        "      Returns\n",
        "      -------\n",
        "      np.ndarray\n",
        "          Windows centered around target points\n",
        "      \"\"\"\n",
        "      if window_size is None:\n",
        "          window_size = self.window_size\n",
        "\n",
        "      if len(time_series.shape) == 1:\n",
        "          time_series = time_series.reshape(1, -1)\n",
        "\n",
        "      batch_size = time_series.shape[0]\n",
        "      n = time_series.shape[1]\n",
        "\n",
        "      # Ensure indices are arrays\n",
        "      if np.isscalar(target_indices):\n",
        "          target_indices = np.array([target_indices])\n",
        "\n",
        "      windows = []\n",
        "      valid_indices = []\n",
        "\n",
        "      for b in range(batch_size):\n",
        "          for idx in target_indices:\n",
        "              # Check if we can create a full window\n",
        "              if idx - window_size >= 0 and idx + window_size <= n:\n",
        "                  window = time_series[b, idx - window_size : idx + window_size]\n",
        "                  windows.append(window)\n",
        "                  valid_indices.append(idx)\n",
        "\n",
        "      return np.array(windows), np.array(valid_indices)\n",
        "\n",
        "  def transform_windows(self, windows, rescale=True):\n",
        "      \"\"\"\n",
        "      Apply transformations to windows (similar to Transform2D but adapted).\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      windows : np.ndarray\n",
        "          Shape (n_windows, window_length)\n",
        "      rescale : bool\n",
        "          Whether to rescale the transformations\n",
        "\n",
        "      Returns\n",
        "      -------\n",
        "      np.ndarray\n",
        "          Shape (n_windows, n_transformations, window_length)\n",
        "      \"\"\"\n",
        "      n_windows, length = windows.shape\n",
        "      windows[windows == 0.0] = 1e-8\n",
        "\n",
        "      # Original\n",
        "      original = windows.copy()\n",
        "\n",
        "      # Squared\n",
        "      squared = np.square(windows)\n",
        "\n",
        "      # Log squared\n",
        "      log_squared = np.log(np.abs(squared) + 1e-8)\n",
        "\n",
        "      # Tanh (or cumsum for change detection)\n",
        "      # Using difference to highlight changes\n",
        "      diff = np.diff(windows, axis=1, prepend=windows[:, 0:1])\n",
        "      tanh_transform = np.tanh(diff)\n",
        "\n",
        "      if rescale:\n",
        "          # Rescale each transformation\n",
        "          original = self._rescale_array(original)\n",
        "          squared = self._rescale_array(squared)\n",
        "          log_squared = self._rescale_array(log_squared)\n",
        "          tanh_transform = self._rescale_array(tanh_transform)\n",
        "\n",
        "      # Stack transformations\n",
        "      result = np.stack([original, squared, log_squared, tanh_transform], axis=1)\n",
        "      return result\n",
        "\n",
        "  def _rescale_array(self, arr):\n",
        "      \"\"\"Rescale array to [0, 1] range.\"\"\"\n",
        "      arr_min = np.min(arr, axis=1, keepdims=True)\n",
        "      arr_max = np.max(arr, axis=1, keepdims=True)\n",
        "      arr_range = arr_max - arr_min\n",
        "      arr_range[arr_range == 0] = 1.0\n",
        "      return (arr - arr_min) / arr_range\n",
        "\n",
        "  def build_focused_model(self, input_length=None):\n",
        "      \"\"\"\n",
        "      Build a CNN model focused on the center point of the window.\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      input_length : int\n",
        "          Length of the input window (default: 2 * window_size)\n",
        "\n",
        "      Returns\n",
        "      -------\n",
        "      keras.Model\n",
        "          The compiled model\n",
        "      \"\"\"\n",
        "      if input_length is None:\n",
        "          input_length = 2 * self.window_size\n",
        "\n",
        "      input_layer = layers.Input(shape=(self.n_transformations, input_length))\n",
        "\n",
        "      # Reshape for Conv2D\n",
        "      x = layers.Reshape((self.n_transformations, input_length, 1))(input_layer)\n",
        "\n",
        "      # First conv block with attention on center\n",
        "      x = layers.Conv2D(32, (2, 5), padding='same', activation='relu')(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "\n",
        "      # Add positional weighting to emphasize center\n",
        "      position_weights = self._create_position_weights(input_length)\n",
        "      x = layers.Lambda(lambda z: z * position_weights)(x)\n",
        "\n",
        "      x = layers.Conv2D(64, (2, 5), padding='same', activation='relu')(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "      x = layers.MaxPooling2D((1, 2))(x)\n",
        "\n",
        "      # Second conv block\n",
        "      x = layers.Conv2D(128, (2, 3), padding='same', activation='relu')(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "      x = layers.MaxPooling2D((1, 2))(x)\n",
        "\n",
        "      # Third conv block focusing on center region\n",
        "      x = layers.Conv2D(256, (2, 3), padding='same', activation='relu')(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "\n",
        "      # Global pooling and dense layers\n",
        "      x = layers.GlobalAveragePooling2D()(x)\n",
        "      x = layers.Dense(128, activation='relu')(x)\n",
        "      x = layers.Dropout(0.5)(x)\n",
        "      x = layers.Dense(64, activation='relu')(x)\n",
        "      x = layers.Dropout(0.3)(x)\n",
        "\n",
        "      # Output layer (binary classification)\n",
        "      output_layer = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "      model = models.Model(input_layer, output_layer, name='focused_cpd')\n",
        "\n",
        "      model.compile(\n",
        "          optimizer='adam',\n",
        "          loss='binary_crossentropy',\n",
        "          metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
        "      )\n",
        "\n",
        "      self.model = model\n",
        "      return model\n",
        "\n",
        "  def _create_position_weights(self, length):\n",
        "      \"\"\"\n",
        "      Create position weights that emphasize the center of the window.\n",
        "      \"\"\"\n",
        "      # Gaussian-like weights centered at the middle\n",
        "      center = length / 2\n",
        "      positions = np.arange(length)\n",
        "      weights = np.exp(-0.5 * ((positions - center) / (length / 6)) ** 2)\n",
        "      weights = weights / np.max(weights)\n",
        "      return tf.constant(weights.reshape(1, 1, length, 1), dtype=tf.float32)\n",
        "\n",
        "  def generate_training_data(self, n_samples=1000, series_length=200,\n",
        "                          changepoint_prob=0.5, noise_level=0.1):\n",
        "      \"\"\"\n",
        "      Generate synthetic training data with known changepoints at specific positions.\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      n_samples : int\n",
        "          Number of training samples\n",
        "      series_length : int\n",
        "          Length of each time series\n",
        "      changepoint_prob : float\n",
        "          Probability of having a changepoint\n",
        "      noise_level : float\n",
        "          Noise level in the data\n",
        "\n",
        "      Returns\n",
        "      -------\n",
        "      tuple\n",
        "          (X_train, y_train, changepoint_positions)\n",
        "      \"\"\"\n",
        "      X_data = []\n",
        "      y_labels = []\n",
        "      cp_positions = []\n",
        "\n",
        "      for _ in range(n_samples):\n",
        "          # Generate base time series\n",
        "          ts = np.zeros(series_length)\n",
        "\n",
        "          # Randomly decide if there's a changepoint\n",
        "          has_cp = np.random.rand() < changepoint_prob\n",
        "\n",
        "          # Choose a target position (center region to ensure full window)\n",
        "          target_pos = np.random.randint(self.window_size,\n",
        "                                        series_length - self.window_size)\n",
        "\n",
        "          if has_cp:\n",
        "              # Create a changepoint at the target position\n",
        "              mean_before = np.random.randn()\n",
        "              mean_after = mean_before + np.random.uniform(0.5, 2.0) * np.random.choice([-1, 1])\n",
        "\n",
        "              ts[:target_pos] = mean_before + np.random.randn(target_pos) * noise_level\n",
        "              ts[target_pos:] = mean_after + np.random.randn(series_length - target_pos) * noise_level\n",
        "\n",
        "              label = 1\n",
        "          else:\n",
        "              # No changepoint - smooth series\n",
        "              ts = np.random.randn() + np.random.randn(series_length) * noise_level\n",
        "              # Add some smooth variation\n",
        "              ts = ts + np.sin(np.linspace(0, 4*np.pi, series_length)) * 0.5\n",
        "              label = 0\n",
        "\n",
        "          # Extract window around target position\n",
        "          window = ts[target_pos - self.window_size : target_pos + self.window_size]\n",
        "\n",
        "          X_data.append(window)\n",
        "          y_labels.append(label)\n",
        "          cp_positions.append(target_pos)\n",
        "\n",
        "      X_data = np.array(X_data)\n",
        "      y_labels = np.array(y_labels)\n",
        "\n",
        "      # Apply transformations\n",
        "      X_transformed = self.transform_windows(X_data)\n",
        "\n",
        "      return X_transformed, y_labels, np.array(cp_positions)\n",
        "\n",
        "  def train(self, X_train, y_train, validation_split=0.2,\n",
        "            epochs=50, batch_size=32, verbose=1):\n",
        "      \"\"\"\n",
        "      Train the model.\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      X_train : np.ndarray\n",
        "          Training data (already transformed)\n",
        "      y_train : np.ndarray\n",
        "          Training labels\n",
        "      validation_split : float\n",
        "          Validation split ratio\n",
        "      epochs : int\n",
        "          Number of epochs\n",
        "      batch_size : int\n",
        "          Batch size\n",
        "      verbose : int\n",
        "          Verbosity level\n",
        "\n",
        "      Returns\n",
        "      -------\n",
        "      History\n",
        "          Training history\n",
        "      \"\"\"\n",
        "      if self.model is None:\n",
        "          self.build_focused_model()\n",
        "\n",
        "      # Shuffle data\n",
        "      X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
        "\n",
        "      # Add callbacks\n",
        "      callbacks = [\n",
        "          tf.keras.callbacks.EarlyStopping(\n",
        "              monitor='val_loss',\n",
        "              patience=10,\n",
        "              restore_best_weights=True\n",
        "          ),\n",
        "          tf.keras.callbacks.ReduceLROnPlateau(\n",
        "              monitor='val_loss',\n",
        "              factor=0.5,\n",
        "              patience=5,\n",
        "              min_lr=1e-6\n",
        "          )\n",
        "      ]\n",
        "\n",
        "      history = self.model.fit(\n",
        "          X_train, y_train,\n",
        "          validation_split=validation_split,\n",
        "          epochs=epochs,\n",
        "          batch_size=batch_size,\n",
        "          callbacks=callbacks,\n",
        "          verbose=verbose\n",
        "      )\n",
        "\n",
        "      return history\n",
        "\n",
        "  def predict_point(self, time_series, target_index):\n",
        "      \"\"\"\n",
        "      Predict if a specific point is a changepoint.\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      time_series : np.ndarray\n",
        "          The time series (1D array)\n",
        "      target_index : int\n",
        "          The index to check for changepoint\n",
        "\n",
        "      Returns\n",
        "      -------\n",
        "      dict\n",
        "          Dictionary with prediction, probability, and validity\n",
        "      \"\"\"\n",
        "      # Check if we can create a valid window\n",
        "      n = len(time_series)\n",
        "      if target_index - self.window_size < 0 or target_index + self.window_size > n:\n",
        "          return {\n",
        "              'valid': False,\n",
        "              'message': f'Cannot create window around index {target_index}',\n",
        "              'prediction': None,\n",
        "              'probability': None\n",
        "          }\n",
        "\n",
        "      # Extract window\n",
        "      window = time_series[target_index - self.window_size :\n",
        "                          target_index + self.window_size]\n",
        "\n",
        "      # Transform window\n",
        "      window_transformed = self.transform_windows(window.reshape(1, -1))\n",
        "\n",
        "      # Predict\n",
        "      prob = self.model.predict(window_transformed, verbose=0)[0, 0]\n",
        "      prediction = int(prob > 0.5)\n",
        "\n",
        "      return {\n",
        "          'valid': True,\n",
        "          'prediction': prediction,\n",
        "          'probability': float(prob),\n",
        "          'is_changepoint': bool(prediction)\n",
        "      }\n",
        "\n",
        "  def evaluate_multiple_points(self, time_series, indices):\n",
        "      \"\"\"\n",
        "      Evaluate multiple points in a time series.\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      time_series : np.ndarray\n",
        "          The time series\n",
        "      indices : list or np.ndarray\n",
        "          Indices to evaluate\n",
        "\n",
        "      Returns\n",
        "      -------\n",
        "      list\n",
        "          List of prediction dictionaries\n",
        "      \"\"\"\n",
        "      results = []\n",
        "      for idx in indices:\n",
        "          result = self.predict_point(time_series, idx)\n",
        "          result['index'] = idx\n",
        "          results.append(result)\n",
        "      return results"
      ],
      "metadata": {
        "id": "VKLC1ayhd2E3"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detector = FocusedChangePointDetector(window_size=50)"
      ],
      "metadata": {
        "id": "-5JKK14Aebr_"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "class DataPreparationPipeline:\n",
        "    \"\"\"\n",
        "    Prepare data from your loader for the FocusedChangePointDetector.\n",
        "    \"\"\"\n",
        "    def __init__(self, loader, window_size=50, n_transformations=4):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        loader : object\n",
        "            Your data loader object with get_series_by_id method\n",
        "        window_size : int\n",
        "            Half-window size for the model (full window = 2 * window_size)\n",
        "        n_transformations : int\n",
        "            Number of transformations to apply\n",
        "        \"\"\"\n",
        "        self.loader = loader\n",
        "        self.window_size = window_size\n",
        "        self.n_transformations = n_transformations\n",
        "        self.detector = None  # Will hold the FocusedChangePointDetector\n",
        "\n",
        "    def prepare_single_series(self, series_id, include_negatives=True, negative_ratio=1.0):\n",
        "        \"\"\"\n",
        "        Prepare data from a single series.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        series_id : int\n",
        "            ID of the series to load\n",
        "        include_negatives : bool\n",
        "            Whether to include non-changepoint samples\n",
        "        negative_ratio : float\n",
        "            Ratio of negative samples to include (relative to 1 positive sample)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple\n",
        "            (windows, labels, metadata)\n",
        "        \"\"\"\n",
        "        # Load data using your loader\n",
        "        series_data, y_value, boundary = self.loader.get_series_by_id(series_id, True)\n",
        "\n",
        "        # Remove the 'period' column if it exists and get just the values\n",
        "        if isinstance(series_data, pd.DataFrame):\n",
        "            if 'period' in series_data.columns:\n",
        "                series_data = series_data.drop('period', axis=1)\n",
        "            # Assuming the main value column is the first one after dropping 'period'\n",
        "            # Adjust this based on your actual column names\n",
        "            values = series_data.iloc[:, 0].values if series_data.shape[1] == 1 else series_data.values.flatten()\n",
        "        else:\n",
        "            values = np.array(series_data).flatten()\n",
        "\n",
        "        windows = []\n",
        "        labels = []\n",
        "        indices = []\n",
        "\n",
        "        # The boundary point is a changepoint (positive sample)\n",
        "        if self._can_create_window(boundary, len(values)):\n",
        "            window = self._extract_window(values, boundary)\n",
        "            windows.append(window)\n",
        "            labels.append(y_value)  # Using y_value from your loader\n",
        "            indices.append(boundary)\n",
        "\n",
        "        # Generate negative samples (non-changepoint windows)\n",
        "        if include_negatives and negative_ratio > 0:\n",
        "            negative_windows, negative_indices = self._generate_negative_samples(\n",
        "                values, boundary, int(negative_ratio)\n",
        "            )\n",
        "            windows.extend(negative_windows)\n",
        "            labels.extend([0] * len(negative_windows))  # 0 for non-changepoints\n",
        "            indices.extend(negative_indices)\n",
        "\n",
        "        if len(windows) == 0:\n",
        "            return None, None, None\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        windows = np.array(windows)\n",
        "        labels = np.array(labels)\n",
        "\n",
        "        # Apply transformations\n",
        "        windows_transformed = self._transform_windows(windows)\n",
        "\n",
        "        metadata = {\n",
        "            'series_id': series_id,\n",
        "            'boundary': boundary,\n",
        "            'indices': indices,\n",
        "            'original_length': len(values)\n",
        "        }\n",
        "\n",
        "        return windows_transformed, labels, metadata\n",
        "\n",
        "    def prepare_multiple_series(self, series_ids=None, n_series=None,\n",
        "                              include_negatives=True, negative_ratio=1.0):\n",
        "        \"\"\"\n",
        "        Prepare data from multiple series.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        series_ids : list\n",
        "            List of series IDs to load. If None, will use range(n_series)\n",
        "        n_series : int\n",
        "            Number of series to load if series_ids is None\n",
        "        include_negatives : bool\n",
        "            Whether to include non-changepoint samples\n",
        "        negative_ratio : float\n",
        "            Ratio of negative samples per positive sample\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple\n",
        "            (X_all, y_all, metadata_all)\n",
        "        \"\"\"\n",
        "        if series_ids is None:\n",
        "            if n_series is None:\n",
        "                raise ValueError(\"Either series_ids or n_series must be provided\")\n",
        "            series_ids = range(n_series)\n",
        "\n",
        "        all_windows = []\n",
        "        all_labels = []\n",
        "        all_metadata = []\n",
        "\n",
        "        for i, sid in enumerate(series_ids):\n",
        "            try:\n",
        "                windows, labels, metadata = self.prepare_single_series(\n",
        "                    sid, include_negatives, negative_ratio\n",
        "                )\n",
        "\n",
        "                if windows is not None:\n",
        "                    all_windows.append(windows)\n",
        "                    all_labels.append(labels)\n",
        "                    all_metadata.append(metadata)\n",
        "\n",
        "                if (i + 1) % 100 == 0:\n",
        "                    print(f\"Processed {i + 1}/{len(series_ids)} series\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing series {sid}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if len(all_windows) == 0:\n",
        "            raise ValueError(\"No valid windows could be created from the data\")\n",
        "\n",
        "        # Concatenate all data\n",
        "        X_all = np.concatenate(all_windows, axis=0)\n",
        "        y_all = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "        print(f\"\\nData preparation complete:\")\n",
        "        print(f\"Total samples: {len(X_all)}\")\n",
        "        print(f\"Positive samples (changepoints): {np.sum(y_all == 1)}\")\n",
        "        print(f\"Negative samples (non-changepoints): {np.sum(y_all == 0)}\")\n",
        "        print(f\"Data shape: {X_all.shape}\")\n",
        "\n",
        "        return X_all, y_all, all_metadata\n",
        "\n",
        "    def prepare_train_test_split(self, series_ids=None, n_series=None,\n",
        "                                test_size=0.2, include_negatives=True,\n",
        "                                negative_ratio=1.0, random_state=42):\n",
        "        \"\"\"\n",
        "        Prepare data and split into train/test sets.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        series_ids : list\n",
        "            List of series IDs to load\n",
        "        n_series : int\n",
        "            Number of series to load if series_ids is None\n",
        "        test_size : float\n",
        "            Proportion of data for testing\n",
        "        include_negatives : bool\n",
        "            Whether to include non-changepoint samples\n",
        "        negative_ratio : float\n",
        "            Ratio of negative samples per positive sample\n",
        "        random_state : int\n",
        "            Random seed for reproducibility\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple\n",
        "            (X_train, X_test, y_train, y_test)\n",
        "        \"\"\"\n",
        "        # Prepare all data\n",
        "        X_all, y_all, metadata = self.prepare_multiple_series(\n",
        "            series_ids, n_series, include_negatives, negative_ratio\n",
        "        )\n",
        "\n",
        "        # Split the data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_all, y_all, test_size=test_size, random_state=random_state,\n",
        "            stratify=y_all  # Maintain class balance in splits\n",
        "        )\n",
        "\n",
        "        print(f\"\\nTrain/Test split:\")\n",
        "        print(f\"Training samples: {len(X_train)} (Positive: {np.sum(y_train == 1)})\")\n",
        "        print(f\"Testing samples: {len(X_test)} (Positive: {np.sum(y_test == 1)})\")\n",
        "\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "    def prepare_for_prediction(self, series_id, target_index=None):\n",
        "        \"\"\"\n",
        "        Prepare a single series for prediction at a specific point.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        series_id : int\n",
        "            ID of the series to load\n",
        "        target_index : int\n",
        "            Specific index to check. If None, uses the boundary point\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple\n",
        "            (window_transformed, target_index, original_values)\n",
        "        \"\"\"\n",
        "        # Load data\n",
        "        series_data, y_value, boundary = self.loader.get_series_by_id(series_id, True)\n",
        "\n",
        "        # Process values\n",
        "        if isinstance(series_data, pd.DataFrame):\n",
        "            if 'period' in series_data.columns:\n",
        "                series_data = series_data.drop('period', axis=1)\n",
        "            values = series_data.iloc[:, 0].values if series_data.shape[1] == 1 else series_data.values.flatten()\n",
        "        else:\n",
        "            values = np.array(series_data).flatten()\n",
        "\n",
        "        # Use boundary as target if not specified\n",
        "        if target_index is None:\n",
        "            target_index = boundary\n",
        "\n",
        "        # Check if we can create a window\n",
        "        if not self._can_create_window(target_index, len(values)):\n",
        "            raise ValueError(f\"Cannot create window around index {target_index}\")\n",
        "\n",
        "        # Extract and transform window\n",
        "        window = self._extract_window(values, target_index)\n",
        "        window_transformed = self._transform_windows(np.array([window]))\n",
        "\n",
        "        return window_transformed, target_index, values\n",
        "\n",
        "    def _can_create_window(self, index, series_length):\n",
        "        \"\"\"Check if a valid window can be created around the index.\"\"\"\n",
        "        return index >= self.window_size and index + self.window_size <= series_length\n",
        "\n",
        "    def _extract_window(self, values, center_index):\n",
        "        \"\"\"Extract a window centered at the given index.\"\"\"\n",
        "        start = center_index - self.window_size\n",
        "        end = center_index + self.window_size\n",
        "        return values[start:end]\n",
        "\n",
        "    def _generate_negative_samples(self, values, boundary, n_negatives):\n",
        "        \"\"\"\n",
        "        Generate negative samples (non-changepoint windows) from the series.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        values : np.ndarray\n",
        "            The time series values\n",
        "        boundary : int\n",
        "            The actual changepoint location to avoid\n",
        "        n_negatives : int\n",
        "            Number of negative samples to generate\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple\n",
        "            (windows, indices)\n",
        "        \"\"\"\n",
        "        series_length = len(values)\n",
        "        windows = []\n",
        "        indices = []\n",
        "\n",
        "        # Define safe zones (away from the actual boundary)\n",
        "        safe_distance = self.window_size // 2  # Minimum distance from boundary\n",
        "\n",
        "        # Potential negative sample positions\n",
        "        potential_indices = []\n",
        "        for idx in range(self.window_size, series_length - self.window_size):\n",
        "            if abs(idx - boundary) > safe_distance:\n",
        "                potential_indices.append(idx)\n",
        "\n",
        "        if len(potential_indices) == 0:\n",
        "            return windows, indices\n",
        "\n",
        "        # Sample negative indices\n",
        "        n_to_sample = min(n_negatives, len(potential_indices))\n",
        "        selected_indices = np.random.choice(potential_indices, n_to_sample, replace=False)\n",
        "\n",
        "        for idx in selected_indices:\n",
        "            window = self._extract_window(values, idx)\n",
        "            windows.append(window)\n",
        "            indices.append(idx)\n",
        "\n",
        "        return windows, indices\n",
        "\n",
        "    def _transform_windows(self, windows):\n",
        "        \"\"\"Apply transformations to windows.\"\"\"\n",
        "        if len(windows.shape) == 1:\n",
        "            windows = windows.reshape(1, -1)\n",
        "\n",
        "        n_windows, length = windows.shape\n",
        "        windows[windows == 0.0] = 1e-8\n",
        "\n",
        "        # Original\n",
        "        original = windows.copy()\n",
        "\n",
        "        # Squared\n",
        "        squared = np.square(windows)\n",
        "\n",
        "        # Log squared\n",
        "        log_squared = np.log(np.abs(squared) + 1e-8)\n",
        "\n",
        "        # Difference (to highlight changes)\n",
        "        diff = np.diff(windows, axis=1, prepend=windows[:, 0:1])\n",
        "        tanh_transform = np.tanh(diff)\n",
        "\n",
        "        # Rescale each transformation\n",
        "        original = self._rescale_array(original)\n",
        "        squared = self._rescale_array(squared)\n",
        "        log_squared = self._rescale_array(log_squared)\n",
        "        tanh_transform = self._rescale_array(tanh_transform)\n",
        "\n",
        "        # Stack transformations\n",
        "        result = np.stack([original, squared, log_squared, tanh_transform], axis=1)\n",
        "        return result\n",
        "\n",
        "    def _rescale_array(self, arr):\n",
        "        \"\"\"Rescale array to [0, 1] range.\"\"\"\n",
        "        arr = arr.copy()\n",
        "        arr_min = np.min(arr, axis=1, keepdims=True)\n",
        "        arr_max = np.max(arr, axis=1, keepdims=True)\n",
        "        arr_range = arr_max - arr_min\n",
        "        arr_range[arr_range == 0] = 1.0\n",
        "        return (arr - arr_min) / arr_range"
      ],
      "metadata": {
        "id": "OV6TjfIOfns7"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prep = DataPreparationPipeline(loader, window_size=50)\n",
        "\n",
        "# Prepare data from multiple series\n",
        "X_train, X_test, y_train, y_test = prep.prepare_train_test_split(\n",
        "    n_series=1000,  # Use 1000 series for training\n",
        "    test_size=0.2,\n",
        "    include_negatives=True,  # Include non-changepoint samples\n",
        "    negative_ratio=2.0  # 2 negative samples per positive\n",
        ")\n",
        "\n",
        "# Train the model (using the FocusedChangePointDetector from before)\n",
        "detector = FocusedChangePointDetector(window_size=50)\n",
        "detector.train(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "FWFViqnrf-Q3",
        "outputId": "49d63c3f-4d3c-4337-93d6-97b03ac5cc1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 100/1000 series\n",
            "Processed 200/1000 series\n",
            "Processed 300/1000 series\n",
            "Processed 400/1000 series\n",
            "Processed 500/1000 series\n",
            "Processed 600/1000 series\n",
            "Processed 700/1000 series\n",
            "Processed 800/1000 series\n",
            "Processed 900/1000 series\n",
            "Processed 1000/1000 series\n",
            "\n",
            "Data preparation complete:\n",
            "Total samples: 3000\n",
            "Positive samples (changepoints): 283\n",
            "Negative samples (non-changepoints): 2717\n",
            "Data shape: (3000, 4, 100)\n",
            "\n",
            "Train/Test split:\n",
            "Training samples: 2400 (Positive: 226)\n",
            "Testing samples: 600 (Positive: 57)\n",
            "Epoch 1/50\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.8611 - auc: 0.5764 - loss: 0.4285 - val_accuracy: 0.9271 - val_auc: 0.4902 - val_loss: 0.2807 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9125 - auc: 0.5594 - loss: 0.3206 - val_accuracy: 0.9271 - val_auc: 0.5132 - val_loss: 0.2738 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8982 - auc: 0.6295 - loss: 0.3271 - val_accuracy: 0.9271 - val_auc: 0.4701 - val_loss: 0.2941 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8952 - auc: 0.6177 - loss: 0.3354 - val_accuracy: 0.9271 - val_auc: 0.5000 - val_loss: 0.3683 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8979 - auc: 0.6335 - loss: 0.3214 - val_accuracy: 0.9271 - val_auc: 0.5178 - val_loss: 0.3923 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9042 - auc: 0.6708 - loss: 0.3022 - val_accuracy: 0.9271 - val_auc: 0.5299 - val_loss: 0.2947 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9039 - auc: 0.6462 - loss: 0.3098 - val_accuracy: 0.9271 - val_auc: 0.4936 - val_loss: 0.2615 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8945 - auc: 0.7627 - loss: 0.2953 - val_accuracy: 0.9271 - val_auc: 0.4604 - val_loss: 0.3237 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9079 - auc: 0.8079 - loss: 0.2509 - val_accuracy: 0.9271 - val_auc: 0.4962 - val_loss: 0.2908 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8962 - auc: 0.8327 - loss: 0.2634 - val_accuracy: 0.9271 - val_auc: 0.4979 - val_loss: 0.5126 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8971 - auc: 0.8436 - loss: 0.2539 - val_accuracy: 0.9271 - val_auc: 0.4865 - val_loss: 0.2987 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8966 - auc: 0.9085 - loss: 0.2173 - val_accuracy: 0.9271 - val_auc: 0.5779 - val_loss: 0.3229 - learning_rate: 0.0010\n",
            "Epoch 13/50\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9115 - auc: 0.9521 - loss: 0.1742 - val_accuracy: 0.9271 - val_auc: 0.5573 - val_loss: 0.3361 - learning_rate: 5.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9580 - auc: 0.9727 - loss: 0.1240 - val_accuracy: 0.9187 - val_auc: 0.5349 - val_loss: 0.3484 - learning_rate: 5.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9676 - auc: 0.9873 - loss: 0.1012 - val_accuracy: 0.9146 - val_auc: 0.5545 - val_loss: 0.3768 - learning_rate: 5.0000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9764 - auc: 0.9920 - loss: 0.0703 - val_accuracy: 0.9271 - val_auc: 0.5153 - val_loss: 0.6242 - learning_rate: 5.0000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9881 - auc: 0.9974 - loss: 0.0426 - val_accuracy: 0.9104 - val_auc: 0.5351 - val_loss: 0.5057 - learning_rate: 5.0000e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e24ea414450>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on a new series\n",
        "window, target_idx, values = prep.prepare_for_prediction(series_id=1001)\n",
        "prob = detector.model.predict(window)[0, 0]\n",
        "print(f\"Changepoint probability: {prob:.4f}\")"
      ],
      "metadata": {
        "id": "Nah8uqVBgfYY",
        "outputId": "e62a61b2-b3af-4e20-b326-b6e0c47b5927",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "Changepoint probability: 0.0585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "imZ28QP1PQLQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiKJODFx-0fR"
      },
      "source": [
        "## Understanding the Data\n",
        "\n",
        "The dataset consists of univariate time series, each containing ~2,000-5,000 values with a designated boundary point. For each time series, you need to determine whether a structural break occurred at this boundary point.\n",
        "\n",
        "The data was downloaded when you setup your local environment and is now available in the `data/` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKHXgvjN-0fS",
        "outputId": "387e305a-3eb4-4afd-ea0e-f2775ec61206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\n",
            "data/X_train.parquet: already exists, file length match\n",
            "data/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\n",
            "data/X_test.reduced.parquet: already exists, file length match\n",
            "data/y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\n",
            "data/y_train.parquet: already exists, file length match\n",
            "data/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n",
            "data/y_test.reduced.parquet: already exists, file length match\n"
          ]
        }
      ],
      "source": [
        "# Load the data simply\n",
        "X_train, y_train, X_test = crunch.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T_JmgMq-0fS"
      },
      "source": [
        "### Understanding `X_train`\n",
        "\n",
        "The training data is structured as a pandas DataFrame with a MultiIndex:\n",
        "\n",
        "**Index Levels:**\n",
        "- `id`: Identifies the unique time series\n",
        "- `time`: The timestep within each time series\n",
        "\n",
        "**Columns:**\n",
        "- `value`: The actual time series value at each timestep\n",
        "- `period`: A binary indicator where `0` represents the **period before** the boundary point, and `1` represents the **period after** the boundary point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "0oRCTnOb-0fS",
        "outputId": "2d0663ba-76b2-4937-d7fc-e6c314784242"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>value</th>\n",
              "      <th>period</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th>time</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
              "      <th>0</th>\n",
              "      <td>0.001858</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.001664</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.004386</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000699</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.002433</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">10000</th>\n",
              "      <th>1890</th>\n",
              "      <td>-0.005903</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1891</th>\n",
              "      <td>0.007295</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1892</th>\n",
              "      <td>0.003527</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1893</th>\n",
              "      <td>0.007218</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1894</th>\n",
              "      <td>0.000034</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>23802099 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               value  period\n",
              "id    time                  \n",
              "0     0     0.001858       0\n",
              "      1    -0.001664       0\n",
              "      2    -0.004386       0\n",
              "      3     0.000699       0\n",
              "      4    -0.002433       0\n",
              "...              ...     ...\n",
              "10000 1890 -0.005903       1\n",
              "      1891  0.007295       1\n",
              "      1892  0.003527       1\n",
              "      1893  0.007218       1\n",
              "      1894  0.000034       1\n",
              "\n",
              "[23802099 rows x 2 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WP39dgx-0fS"
      },
      "source": [
        "### Understanding `y_train`\n",
        "\n",
        "This is a simple `pandas.Series` that tells if a dataset id has a structural breakpoint or not.\n",
        "\n",
        "**Index:**\n",
        "- `id`: the ID of the dataset\n",
        "\n",
        "**Value:**\n",
        "- `structural_breakpoint`: Boolean indicating whether a structural break occurred (`True`) or not (`False`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "dPsQPdIj-0fT",
        "outputId": "acd28eab-afd8-44c7-d229-d7e414f2e3c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "id\n",
              "0         True\n",
              "1         True\n",
              "2        False\n",
              "3         True\n",
              "4        False\n",
              "         ...  \n",
              "9996     False\n",
              "9997      True\n",
              "9998     False\n",
              "9999     False\n",
              "10000     True\n",
              "Name: structural_breakpoint, Length: 10001, dtype: bool"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oSS08Ks-0fT"
      },
      "source": [
        "### Understanding `X_test`\n",
        "\n",
        "The test data is provided as a **`list` of `pandas.DataFrame`s** with the same format as [`X_train`](#understanding-X_test).\n",
        "\n",
        "It is structured as a list to encourage processing records one by one, which will be mandatory in the `infer()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ErbKAs--0fT",
        "outputId": "5ce46294-1824-4067-be90-547189e90be3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of datasets: 101\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of datasets:\", len(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "M_dTYXms-0fT",
        "outputId": "b3ee6375-995f-47f6-f6e5-7f08d9838820"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>value</th>\n",
              "      <th>period</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th>time</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"11\" valign=\"top\">10001</th>\n",
              "      <th>0</th>\n",
              "      <td>-0.020657</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.005894</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.003052</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.000590</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.009887</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2517</th>\n",
              "      <td>0.005084</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2518</th>\n",
              "      <td>-0.024414</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2519</th>\n",
              "      <td>-0.014986</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2520</th>\n",
              "      <td>0.012999</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2521</th>\n",
              "      <td>-0.022138</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2522 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               value  period\n",
              "id    time                  \n",
              "10001 0    -0.020657       0\n",
              "      1    -0.005894       0\n",
              "      2    -0.003052       0\n",
              "      3    -0.000590       0\n",
              "      4     0.009887       0\n",
              "...              ...     ...\n",
              "      2517  0.005084       1\n",
              "      2518 -0.024414       1\n",
              "      2519 -0.014986       1\n",
              "      2520  0.012999       1\n",
              "      2521 -0.022138       1\n",
              "\n",
              "[2522 rows x 2 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgulFOGX-0fT"
      },
      "source": [
        "## Strategy Implementation\n",
        "\n",
        "There are multiple approaches you can take to detect structural breaks:\n",
        "\n",
        "1. **Statistical Tests**: Compare distributions before and after the boundary point;\n",
        "2. **Feature Engineering**: Extract features from both segments for comparison;\n",
        "3. **Time Series Modeling**: Detect deviations from expected patterns;\n",
        "4. **Machine Learning**: Train models to recognize break patterns from labeled examples.\n",
        "\n",
        "The baseline implementation below uses a simple statistical approach: a t-test to compare the distributions before and after the boundary point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLfYIXlz-0fT"
      },
      "source": [
        "### The `train()` Function\n",
        "\n",
        "In this function, you build and train your model for making inferences on the test data. Your model must be stored in the `model_directory_path`.\n",
        "\n",
        "The baseline implementation below doesn't require a pre-trained model, as it uses a statistical test that will be computed at inference time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-18T10:04:00.459399Z",
          "start_time": "2024-11-18T10:04:00.455716Z"
        },
        "id": "xQwWDC6M-0fT"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.Series,\n",
        "    model_directory_path: str,\n",
        "):\n",
        "    # For our baseline t-test approach, we don't need to train a model\n",
        "    # This is essentially an unsupervised approach calculated at inference time\n",
        "    model = None\n",
        "\n",
        "    # You could enhance this by training an actual model, for example:\n",
        "    # 1. Extract features from before/after segments of each time series\n",
        "    # 2. Train a classifier using these features and y_train labels\n",
        "    # 3. Save the trained model\n",
        "\n",
        "    joblib.dump(model, os.path.join(model_directory_path, 'model.joblib'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n-jboJH-0fU"
      },
      "source": [
        "### The `infer()` Function\n",
        "\n",
        "In the inference function, your trained model (if any) is loaded and used to make predictions on test data.\n",
        "\n",
        "**Important workflow:**\n",
        "1. Load your model;\n",
        "2. Use the `yield` statement to signal readiness to the runner;\n",
        "3. Process each dataset one by one within the for loop;\n",
        "4. For each dataset, use `yield prediction` to return your prediction.\n",
        "\n",
        "**Note:** The datasets can only be iterated once!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-18T10:03:59.120294Z",
          "start_time": "2024-11-18T10:03:59.114830Z"
        },
        "id": "r1b7hRkl-0fU"
      },
      "outputs": [],
      "source": [
        "def infer(\n",
        "    X_test: typing.Iterable[pd.DataFrame],\n",
        "    model_directory_path: str,\n",
        "):\n",
        "    model = joblib.load(os.path.join(model_directory_path, 'model.joblib'))\n",
        "\n",
        "    yield  # Mark as ready\n",
        "\n",
        "    # X_test can only be iterated once.\n",
        "    # Before getting the next dataset, you must predict the current one.\n",
        "    for dataset in X_test:\n",
        "        # Baseline approach: Compute t-test between values before and after boundary point\n",
        "        # The negative p-value is used as our score - smaller p-values (larger negative numbers)\n",
        "        # indicate more evidence against the null hypothesis that distributions are the same,\n",
        "        # suggesting a structural break\n",
        "        def t_test(u: pd.DataFrame):\n",
        "            return -scipy.stats.ttest_ind(\n",
        "                u[\"value\"][u[\"period\"] == 0],  # Values before boundary point\n",
        "                u[\"value\"][u[\"period\"] == 1],  # Values after boundary point\n",
        "            ).pvalue\n",
        "\n",
        "        prediction = t_test(dataset)\n",
        "        yield prediction  # Send the prediction for the current dataset\n",
        "\n",
        "        # Note: This baseline approach uses a t-test to compare the distributions\n",
        "        # before and after the boundary point. A smaller p-value (larger negative number)\n",
        "        # suggests stronger evidence that the distributions are different,\n",
        "        # indicating a potential structural break."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W0Kl9CA-0fU"
      },
      "source": [
        "## Local testing\n",
        "\n",
        "To make sure your `train()` and `infer()` function are working properly, you can call the `crunch.test()` function that will reproduce the cloud environment locally. <br />\n",
        "Even if it is not perfect, it should give you a quick idea if your model is working properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDZeP-4--0fU"
      },
      "outputs": [],
      "source": [
        "crunch.test(\n",
        "    # Uncomment to disable the train\n",
        "    # force_first_train=False,\n",
        "\n",
        "    # Uncomment to disable the determinism check\n",
        "    # no_determinism_check=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV_5CKs--0fU"
      },
      "source": [
        "## Results\n",
        "\n",
        "Once the local tester is done, you can preview the result stored in `data/prediction.parquet`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ly5q68sA-0fU"
      },
      "outputs": [],
      "source": [
        "prediction = pd.read_parquet(\"data/prediction.parquet\")\n",
        "prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oP-NLGh-0fU"
      },
      "source": [
        "### Local scoring\n",
        "\n",
        "You can call the function that the system uses to estimate your score locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyCrjpzv-0fU"
      },
      "outputs": [],
      "source": [
        "# Load the targets\n",
        "target = pd.read_parquet(\"data/y_test.reduced.parquet\")[\"structural_breakpoint\"]\n",
        "\n",
        "# Call the scoring function\n",
        "sklearn.metrics.roc_auc_score(\n",
        "    target,\n",
        "    prediction,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AE1i3pR-0fV"
      },
      "source": [
        "# Submit your Notebook\n",
        "\n",
        "To submit your work, you must:\n",
        "1. Download your Notebook from Colab\n",
        "2. Upload it to the platform\n",
        "3. Create a run to validate it\n",
        "\n",
        "### >> https://hub.crunchdao.com/competitions/structural-break/submit/notebook\n",
        "\n",
        "![Download and Submit Notebook](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/documentation/animations/download-and-submit-notebook.gif)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}