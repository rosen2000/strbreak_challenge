{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvWIItAe-0fN"
      },
      "source": [
        "[![Open In Colab](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/documentation/badge/open-in-colab.svg)](https://colab.research.google.com/github/crunchdao/quickstarters/blob/master/competitions/structural-break/quickstarters/baseline/baseline.ipynb)\n",
        "[![Open In Kaggle](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/documentation/badge/open-in-kaggle.svg)](https://www.kaggle.com/code/crunchdao/structural-break-baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DUeixiC_IJM",
        "outputId": "28dd0ec7-aab1-4f46-8128-337af00d7db0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "crunch-cli, version 7.4.0\n",
            "main.py: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/submissions/23863/main.py (6729 bytes)\n",
            "requirements.txt: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/submissions/23863/requirements.original.txt (194 bytes)\n",
            "resources/cnnlstm_model.py: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/models/25094/cnnlstm_model.py (19374 bytes)\n",
            "resources/lgbm_model.py: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/models/25094/lgbm_model.py (17887 bytes)\n",
            "resources/lightgbm_features_cache.pkl: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/models/25094/lightgbm_features_cache.pkl (5784251 bytes)\n",
            "resources/tft_model.py: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/models/25094/tft_model.py (20804 bytes)\n",
            "data/X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\n",
            "data/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\n",
            "data/y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\n",
            "data/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n",
            "                                \n",
            "---\n",
            "Success! Your environment has been correctly setup.\n",
            "Next recommended actions:\n",
            "1. Load the Crunch Toolings: `crunch = crunch.load_notebook()`\n",
            "2. Execute the cells with your code\n",
            "3. Run a test: `crunch.test()`\n",
            "4. Download and submit your code to the platform!\n"
          ]
        }
      ],
      "source": [
        "%pip install crunch-cli --upgrade --quiet --progress-bar off\n",
        "!crunch setup-notebook structural-break MOc241YzAb1h8V4n463c9UXp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-18T09:52:21.302334Z",
          "start_time": "2024-11-18T09:52:18.268241Z"
        },
        "id": "MKqz-6Zw-0fR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import typing\n",
        "\n",
        "# Import your dependencies\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import sklearn.metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjD_WSAS-0fR",
        "outputId": "4e58e59a-808a-4b19-b731-e9d7bf818dee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded inline runner with module: <module '__main__'>\n",
            "\n",
            "cli version: 7.4.0\n",
            "available ram: 12.67 gb\n",
            "available cpu: 2 core\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "import crunch\n",
        "\n",
        "# Load the Crunch Toolings\n",
        "crunch = crunch.load_notebook()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install autocpd"
      ],
      "metadata": {
        "id": "RHQWxyiDOWzG",
        "outputId": "232cdab9-3e73-4965-c5d6-ed0540360800",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autocpd\n",
            "  Downloading autocpd-0.1.5-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from autocpd) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from autocpd) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from autocpd) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from autocpd) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from autocpd) (1.16.1)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from autocpd) (0.14.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->autocpd) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->autocpd) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->autocpd) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->autocpd) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->autocpd) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->autocpd) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->autocpd) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->autocpd) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->autocpd) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->autocpd) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->autocpd) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->autocpd) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->autocpd) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->autocpd) (1.17.0)\n",
            "Downloading autocpd-0.1.5-py3-none-any.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m120.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: autocpd\n",
            "Successfully installed autocpd-0.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow_docs"
      ],
      "metadata": {
        "id": "W_pdFvl-Oc8O",
        "outputId": "a5b0d8ae-0f91-4445-97bc-4cf62615a9fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_docs\n",
            "  Downloading tensorflow_docs-2025.2.19.33219-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting astor (from tensorflow_docs)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from tensorflow_docs) (1.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from tensorflow_docs) (3.1.6)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from tensorflow_docs) (5.10.4)\n",
            "Requirement already satisfied: protobuf>=3.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow_docs) (5.29.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from tensorflow_docs) (6.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->tensorflow_docs) (3.0.2)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->tensorflow_docs) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->tensorflow_docs) (4.25.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from nbformat->tensorflow_docs) (5.8.1)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.11/dist-packages (from nbformat->tensorflow_docs) (5.7.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->tensorflow_docs) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->tensorflow_docs) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->tensorflow_docs) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->tensorflow_docs) (0.26.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->tensorflow_docs) (4.3.8)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat->tensorflow_docs) (4.14.1)\n",
            "Downloading tensorflow_docs-2025.2.19.33219-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.7/182.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Installing collected packages: astor, tensorflow_docs\n",
            "Successfully installed astor-0.8.1 tensorflow_docs-2025.2.19.33219\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_docs.modeling as tfdoc_model\n",
        "import tensorflow_docs.plots as tfdoc_plot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from autocpd.neuralnetwork import compile_and_fit, general_simple_nn,general_deep_nn  # *** Changed to simple_nn ***\n",
        "from autocpd.utils import Transform2D2TR\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "oJlNxj-bOZcL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import tensorflow_docs.modeling as tfdoc_model\n",
        "import tensorflow_docs.plots as tfdoc_plot\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from autocpd.neuralnetwork import compile_and_fit, general_deep_nn\n",
        "from autocpd.utils import DataSetGen, Transform2D2TR"
      ],
      "metadata": {
        "id": "wdr8WQ4ZPg9X"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Time Series Data Loading and Iteration Utilities\n",
        "\n",
        "This module provides utilities for loading and iterating through time series data\n",
        "with boundary point detection and cropping functionality.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Iterator, Tuple, Optional, Dict, Any\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "class TimeSeriesDataLoader:\n",
        "    \"\"\"\n",
        "    A utility class for loading and iterating through time series data.\n",
        "\n",
        "    This class handles:\n",
        "    - Loading multiindex time series data from parquet files\n",
        "    - Iterating through individual time series\n",
        "    - Boundary point detection (where 'period' changes from 0 to 1)\n",
        "    - Cropping time series around boundary points\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 x_path: str = \"data/X_train.parquet\",\n",
        "                 y_path: str = \"data/y_train.parquet\",\n",
        "                 window_size: int = 249,\n",
        "                 keep_period: bool = True):\n",
        "        \"\"\"\n",
        "        Initialize the TimeSeriesDataLoader.\n",
        "\n",
        "        Args:\n",
        "            x_path: Path to the X data parquet file\n",
        "            y_path: Path to the y data parquet file\n",
        "            window_size: Number of timesteps before and after boundary point to include\n",
        "        \"\"\"\n",
        "        self.x_path = x_path\n",
        "        self.y_path = y_path\n",
        "        self.window_size = window_size\n",
        "        self.keep_period = keep_period\n",
        "        # Load data\n",
        "        self._load_data()\n",
        "\n",
        "    def _load_data(self):\n",
        "        \"\"\"Load the data from parquet files.\"\"\"\n",
        "        print(f\"Loading X data from {self.x_path}...\")\n",
        "        self.X = pd.read_parquet(self.x_path)\n",
        "\n",
        "        print(f\"Loading y data from {self.y_path}...\")\n",
        "        self.y = pd.read_parquet(self.y_path)\n",
        "\n",
        "        # Ensure X has proper MultiIndex\n",
        "        if not isinstance(self.X.index, pd.MultiIndex):\n",
        "            if 'id' in self.X.columns and 'time' in self.X.columns:\n",
        "                self.X.set_index(['id', 'time'], inplace=True)\n",
        "            else:\n",
        "                # Try to infer from index structure\n",
        "                if hasattr(self.X.index, 'names') and len(self.X.index.names) == 2:\n",
        "                    self.X.index.names = ['id', 'time']\n",
        "                else:\n",
        "                    raise ValueError(\"X data must have MultiIndex with 'id' and 'time' levels\")\n",
        "\n",
        "        # Process y data\n",
        "        if \"structural_breakpoint\" in self.y.columns:\n",
        "            self.y = self.y[\"structural_breakpoint\"]\n",
        "        else:\n",
        "            self.y = self.y.squeeze()\n",
        "\n",
        "        # Get unique series IDs\n",
        "        self.series_ids = self.X.index.get_level_values('id').unique()\n",
        "        print(f\"Loaded {len(self.series_ids)} time series\")\n",
        "\n",
        "    def get_series_iterator(self,\n",
        "                          crop_around_boundary: bool = False,\n",
        "                          include_y: bool = True) -> Iterator[Tuple[str, pd.DataFrame, Optional[float]]]:\n",
        "        \"\"\"\n",
        "        Get an iterator that yields individual time series.\n",
        "\n",
        "        Args:\n",
        "            crop_around_boundary: If True, crop each series around its boundary point\n",
        "            include_y: If True, include the corresponding y value\n",
        "\n",
        "        Yields:\n",
        "            Tuple of (series_id, series_data, y_value)\n",
        "        \"\"\"\n",
        "        for series_id in self.series_ids:\n",
        "            series_data = self.X.loc[series_id].copy()\n",
        "\n",
        "            if crop_around_boundary:\n",
        "                series_data = self._crop_around_boundary(series_id, series_data)\n",
        "                if series_data is None:\n",
        "                    continue  # Skip series that couldn't be cropped\n",
        "\n",
        "            y_value = self.y.loc[series_id] if include_y else None\n",
        "            if self.keep_period == False:\n",
        "                series_data = series_data.drop(\"period\",axis=1)\n",
        "            yield series_id, series_data, y_value\n",
        "\n",
        "    def _find_boundary_point(self, series_data: pd.DataFrame) -> Optional[int]:\n",
        "        \"\"\"\n",
        "        Find the boundary point in a time series.\n",
        "\n",
        "        Args:\n",
        "            series_data: DataFrame containing the time series data\n",
        "\n",
        "        Returns:\n",
        "            Index of the boundary point, or None if not found\n",
        "        \"\"\"\n",
        "        if 'period' not in series_data.columns:\n",
        "            return None\n",
        "\n",
        "        period = series_data['period'].values\n",
        "\n",
        "        # Find the first index where period goes from 0 to 1\n",
        "        bp_candidates = np.where((period[:-1] == 0) & (period[1:] == 1))[0]\n",
        "\n",
        "        if len(bp_candidates) == 0:\n",
        "            # Fallback: just where period == 1\n",
        "            bp_candidates = np.where(period == 1)[0]\n",
        "\n",
        "        if len(bp_candidates) == 0:\n",
        "            return None\n",
        "\n",
        "        # Return the first boundary point\n",
        "        bp = bp_candidates[0] + 1 if (period[:-1] == 0).any() else bp_candidates[0]\n",
        "        return bp\n",
        "\n",
        "    def _crop_around_boundary(self, series_id: str, series_data: pd.DataFrame) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Crop a time series around its boundary point.\n",
        "\n",
        "        Args:\n",
        "            series_id: ID of the time series\n",
        "            series_data: DataFrame containing the time series data\n",
        "\n",
        "        Returns:\n",
        "            Cropped DataFrame or None if cropping is not possible\n",
        "        \"\"\"\n",
        "        bp = self._find_boundary_point(series_data)\n",
        "\n",
        "        if bp is None:\n",
        "            print(f\"Warning: No boundary point found for series {series_id}\")\n",
        "            return None\n",
        "\n",
        "        start = bp - self.window_size\n",
        "        end = bp + self.window_size-1\n",
        "\n",
        "        if start < 0 or end >= len(series_data):\n",
        "            print(f\"Warning: Series {series_id} too short for cropping (length: {len(series_data)}, required: {2*self.window_size + 1})\")\n",
        "            return None\n",
        "\n",
        "        cropped_data = series_data.iloc[start:end+1].copy()\n",
        "\n",
        "        # Reset the time index to start from 1\n",
        "        cropped_data.index = pd.RangeIndex(1, len(cropped_data) + 1)\n",
        "\n",
        "        return cropped_data\n",
        "\n",
        "    def get_series_by_id(self, series_id: str, crop_around_boundary: bool = False) -> Tuple[pd.DataFrame, float]:\n",
        "        \"\"\"\n",
        "        Get a specific time series by ID.\n",
        "\n",
        "        Args:\n",
        "            series_id: ID of the time series to retrieve\n",
        "            crop_around_boundary: If True, crop around boundary point\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (series_data, y_value)\n",
        "        \"\"\"\n",
        "        if series_id not in self.series_ids:\n",
        "            raise ValueError(f\"Series ID {series_id} not found in dataset\")\n",
        "\n",
        "        series_data = self.X.loc[series_id].copy()\n",
        "        if crop_around_boundary:\n",
        "            boundary_point = self.window_size +1\n",
        "        else:\n",
        "            boundary_point = self._find_boundary_point(series_data)\n",
        "        if crop_around_boundary:\n",
        "            series_data = self._crop_around_boundary(series_id, series_data)\n",
        "            if series_data is None:\n",
        "                raise ValueError(f\"Could not crop series {series_id} around boundary point\")\n",
        "        if self.keep_period == False:\n",
        "                series_data = series_data.drop(\"period\",axis=1)\n",
        "        y_value = self.y.loc[series_id]\n",
        "        return series_data, y_value, boundary_point\n",
        "\n",
        "    def get_all_cropped_series(self) -> Tuple[pd.DataFrame, pd.Series]:\n",
        "        \"\"\"\n",
        "        Get all time series cropped around their boundary points.\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (X_cropped, y_cropped) where X_cropped is a MultiIndex DataFrame\n",
        "        \"\"\"\n",
        "        cropped_series = []\n",
        "        kept_ids = []\n",
        "\n",
        "        for series_id, series_data, y_value in self.get_series_iterator(crop_around_boundary=True):\n",
        "            cropped_data = self._crop_around_boundary(series_id, series_data)\n",
        "            if cropped_data is not None:\n",
        "                # Set proper MultiIndex for the cropped data\n",
        "                cropped_data.index = pd.MultiIndex.from_product(\n",
        "                    [[series_id], range(1, len(cropped_data) + 1)],\n",
        "                    names=['id', 'time']\n",
        "                )\n",
        "                cropped_series.append(cropped_data)\n",
        "                kept_ids.append(series_id)\n",
        "\n",
        "        if not cropped_series:\n",
        "            raise ValueError(\"No series could be cropped successfully\")\n",
        "\n",
        "        X_cropped = pd.concat(cropped_series)\n",
        "        y_cropped = self.y.loc[kept_ids]\n",
        "        if self.keep_period == False:\n",
        "                X_cropped = X_cropped.drop(\"period\",axis=1)\n",
        "        print(f\"Successfully cropped {len(kept_ids)} out of {len(self.series_ids)} series\")\n",
        "\n",
        "        return X_cropped, y_cropped\n",
        "\n",
        "    def get_series_info(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Get information about the loaded dataset.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with dataset information\n",
        "        \"\"\"\n",
        "        info = {\n",
        "            'total_series': len(self.series_ids),\n",
        "            'columns': list(self.X.columns),\n",
        "            'total_timesteps': len(self.X),\n",
        "            'avg_series_length': len(self.X) / len(self.series_ids),\n",
        "            'window_size': self.window_size,\n",
        "            'y_unique_values': self.y.unique().tolist(),\n",
        "            'y_distribution': self.y.value_counts().to_dict()\n",
        "        }\n",
        "        return info\n",
        "\n",
        "    def save_cropped_data(self,\n",
        "                         x_output_path: str = \"X_train_cropped.parquet\",\n",
        "                         y_output_path: str = \"y_train_cropped.parquet\"):\n",
        "        \"\"\"\n",
        "        Save all cropped time series to parquet files.\n",
        "\n",
        "        Args:\n",
        "            x_output_path: Path to save cropped X data\n",
        "            y_output_path: Path to save cropped y data\n",
        "        \"\"\"\n",
        "        X_cropped, y_cropped = self.get_all_cropped_series()\n",
        "\n",
        "        X_cropped.to_parquet(x_output_path)\n",
        "        y_cropped.to_frame().to_parquet(y_output_path)\n",
        "\n",
        "        print(f\"Saved cropped data to {x_output_path} and {y_output_path}\")\n",
        "\n",
        "    def get_series_before_after_by_id(self, series_id: str, crop_around_boundary: bool = False) -> Tuple[pd.DataFrame, pd.DataFrame, float, int]:\n",
        "        \"\"\"\n",
        "        Get a specific time series by ID, returning two series: before and after the boundary point.\n",
        "\n",
        "        Args:\n",
        "            series_id: ID of the time series to retrieve\n",
        "            crop_around_boundary: If True, crop to window_size timesteps before and after boundary point\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (series_before, series_after, y_value, boundary_point_index)\n",
        "        \"\"\"\n",
        "        if series_id not in self.series_ids:\n",
        "            raise ValueError(f\"Series ID {series_id} not found in dataset\")\n",
        "\n",
        "        series_data = self.X.loc[series_id].copy()\n",
        "        boundary_point = self._find_boundary_point(series_data)\n",
        "\n",
        "        if boundary_point is None:\n",
        "            raise ValueError(f\"No boundary point found for series {series_id}\")\n",
        "\n",
        "        if crop_around_boundary:\n",
        "            # Crop to window_size before and after boundary point\n",
        "            start_before = max(0, boundary_point - self.window_size)\n",
        "            end_before = boundary_point\n",
        "            start_after = boundary_point\n",
        "            end_after = min(len(series_data), boundary_point + self.window_size)\n",
        "        else:\n",
        "            # Use full series before and after boundary point\n",
        "            start_before = 0\n",
        "            end_before = boundary_point\n",
        "            start_after = boundary_point\n",
        "            end_after = len(series_data)\n",
        "\n",
        "        # Extract before and after series\n",
        "        series_before = series_data.iloc[start_before:end_before].copy()\n",
        "        series_after = series_data.iloc[start_after:end_after].copy()\n",
        "\n",
        "        # Reset indices to start from 1\n",
        "        series_before.index = pd.RangeIndex(1, len(series_before) + 1)\n",
        "        series_after.index = pd.RangeIndex(1, len(series_after) + 1)\n",
        "\n",
        "        # Remove period column if not keeping it\n",
        "        if self.keep_period == False:\n",
        "            series_before = series_before.drop(\"period\", axis=1)\n",
        "            series_after = series_after.drop(\"period\", axis=1)\n",
        "\n",
        "        y_value = self.y.loc[series_id]\n",
        "        return series_before, series_after, y_value, boundary_point\n",
        "\n",
        "\n",
        "# Convenience functions for quick access\n",
        "def load_timeseries_data(x_path: str = \"data/X_train.parquet\",\n",
        "                        y_path: str = \"data/y_train.parquet\") -> TimeSeriesDataLoader:\n",
        "    \"\"\"\n",
        "    Convenience function to create a TimeSeriesDataLoader instance.\n",
        "\n",
        "    Args:\n",
        "        x_path: Path to X data\n",
        "        y_path: Path to y data\n",
        "\n",
        "    Returns:\n",
        "        TimeSeriesDataLoader instance\n",
        "    \"\"\"\n",
        "    return TimeSeriesDataLoader(x_path, y_path)\n",
        "\n",
        "\n",
        "def iterate_series(loader: TimeSeriesDataLoader,\n",
        "                  crop_around_boundary: bool = False) -> Iterator[Tuple[str, pd.DataFrame, float]]:\n",
        "    \"\"\"\n",
        "    Convenience function to iterate through all series.\n",
        "\n",
        "    Args:\n",
        "        loader: TimeSeriesDataLoader instance\n",
        "        crop_around_boundary: Whether to crop around boundary points\n",
        "\n",
        "    Yields:\n",
        "        Tuple of (series_id, series_data, y_value)\n",
        "    \"\"\"\n",
        "    for series_id, series_data, y_value in loader.get_series_iterator(crop_around_boundary=crop_around_boundary):\n",
        "        yield series_id, series_data, y_value\n",
        "\n"
      ],
      "metadata": {
        "id": "zcrh6cRAOl8W"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_series = []\n",
        "all_labels = []\n",
        "for i in range (0,10000,1):\n",
        "  series_data,y_value,boundary = loader.get_series_by_id(i,True)\n",
        "  series_data_clean = series_data[\"value\"]\n",
        "  all_series.append(series_data_clean)\n",
        "  all_labels.append(y_value)\n",
        "data_x = np.array(all_series)  # Shape: (num_ids, 498)\n",
        "labels = np.array(all_labels)   # Shape: (num_ids,)\n",
        "\n",
        "print(f\"Data shape: {data_x.shape}\")\n",
        "print(f\"Labels shape: {labels.shape}\")\n",
        "\n",
        "true_indices = np.where(labels == True)[0]\n",
        "false_indices = np.where(labels == False)[0]\n",
        "\n",
        "print(f\"True samples: {len(true_indices)}\")   # ~3333\n",
        "print(f\"False samples: {len(false_indices)}\") # ~6666\n",
        "\n",
        "from sklearn.utils import resample\n",
        "\n",
        "false_balanced = resample(false_indices,\n",
        "                         replace=False,\n",
        "                         n_samples=len(true_indices),\n",
        "                         random_state=42)\n",
        "\n",
        "# Combine balanced indices\n",
        "balanced_indices = np.concatenate([true_indices, false_balanced])\n",
        "np.random.shuffle(balanced_indices)\n",
        "\n",
        "# Create balanced dataset by indexing your existing arrays\n",
        "data_x_balanced = data_x[balanced_indices]\n",
        "labels_balanced = labels[balanced_indices].astype(int)  # Convert True/False to 1/0\n",
        "\n",
        "print(f\"Balanced data shape: {data_x_balanced.shape}\")  # Should be (~6666, 498)\n",
        "print(f\"Balanced labels shape: {labels_balanced.shape}\")\n",
        "print(f\"Balanced class distribution: {np.bincount(labels_balanced)}\")\n",
        "\n",
        "# Now use data_x_balanced and labels_balanced for your model\n",
        "data_x = data_x_balanced\n",
        "labels = labels_balanced\n",
        "\n",
        "# Method 1: Per-sample min-max normalization (RECOMMENDED)\n",
        "# This preserves the shape of each time series while normalizing to [0,1]\n",
        "def normalize_per_sample(data):\n",
        "    normalized = np.zeros_like(data)\n",
        "    for i in range(data.shape[0]):\n",
        "        sample = data[i]\n",
        "        sample_min = sample.min()\n",
        "        sample_max = sample.max()\n",
        "        if sample_max > sample_min:  # Avoid division by zero\n",
        "            normalized[i] = (sample - sample_min) / (sample_max - sample_min)\n",
        "        else:\n",
        "            normalized[i] = np.zeros_like(sample)  # Flat signal = all zeros\n",
        "    return normalized\n",
        "\n",
        "# Apply the better normalization\n",
        "data_x_norm = normalize_per_sample(data_x_balanced)\n",
        "\n",
        "# Check if this fixes the variation issue\n",
        "print(\"After per-sample normalization:\")\n",
        "print(f\"First sample std: {data_x_norm[0].std():.4f}\")\n",
        "print(f\"Sample means: {data_x_norm.mean(axis=1)[:5]}\")\n",
        "print(f\"Sample stds: {data_x_norm.std(axis=1)[:5]}\")\n",
        "print(f\"Overall range: [{data_x_norm.min():.4f}, {data_x_norm.max():.4f}]\")\n",
        "\n",
        "# Split and train\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    data_x_norm, labels_balanced.reshape(-1, 1), train_size=0.8, random_state=42\n",
        ")\n",
        "\n",
        "x_train = Transform2D2TR(x_train, rescale=False, times=3)\n",
        "x_test = Transform2D2TR(x_test, rescale=False, times=3)"
      ],
      "metadata": {
        "id": "WZdHtEYiO04a",
        "outputId": "24e9c3bd-05b3-4047-952f-b9e1bf5145d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: (10000, 498)\n",
            "Labels shape: (10000,)\n",
            "True samples: 2908\n",
            "False samples: 7092\n",
            "Balanced data shape: (5816, 498)\n",
            "Balanced labels shape: (5816,)\n",
            "Balanced class distribution: [2908 2908]\n",
            "After per-sample normalization:\n",
            "First sample std: 0.1193\n",
            "Sample means: [0.64402393 0.44044606 0.42643469 0.44594874 0.39690033]\n",
            "Sample stds: [0.11928432 0.18106374 0.11487443 0.06359554 0.08363304]\n",
            "Overall range: [0.0000, 1.0000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "epochs = 100\n",
        "batch_size = 64\n",
        "dropout_rate = 0.3\n",
        "n_filter = 16\n",
        "n = x_train.shape[-1]\n",
        "num_tran = x_train.shape[1]\n",
        "kernel_size = (num_tran // 2, 10)\n",
        "num_classes = 2\n",
        "logdir = Path(\"tensorboard_logs\", \"Trial\")\n",
        "# %%\n",
        "num_resblock = 3\n",
        "model_name = \"Toshko\"\n",
        "print(model_name)\n",
        "# build the model\n",
        "m = np.array([50, 40, 30, 20, 10])\n",
        "model = general_deep_nn(\n",
        "    n,\n",
        "    num_tran,\n",
        "    kernel_size,\n",
        "    n_filter,\n",
        "    dropout_rate,\n",
        "    num_classes,\n",
        "    num_resblock,\n",
        "    m,\n",
        "    5,\n",
        "    model_name=model_name,\n",
        ")\n",
        "model.summary()\n",
        "\n",
        "size_histories = {}\n",
        "epochdots = tfdoc_model.EpochDots()\n",
        "size_histories[model_name] = compile_and_fit(\n",
        "    model,\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size,\n",
        "    learning_rate,\n",
        "    model_name,\n",
        "    logdir,\n",
        "    epochdots,\n",
        "    validation_split=0.25,\n",
        "    max_epochs=epochs,\n",
        ")\n",
        "plotter = tfdoc_plot.HistoryPlotter(metric=\"accuracy\", smoothing_std=10)\n",
        "plt.figure(figsize=(10, 8))\n",
        "plotter.plot(size_histories)\n",
        "acc_name = model_name + \"+acc.png\"\n",
        "acc_path = Path(logdir, model_name, acc_name)\n",
        "plt.savefig(acc_path)\n",
        "\n",
        "\n",
        "model_path = Path(logdir, model_name, \"model\")\n",
        "model.save(model_path)\n",
        "\n",
        "#  Confusion Matrix\n",
        "model_pred = model.evaluate(x_test, y_test, verbose=2)\n",
        "y_prob = np.max(model.predict(x_test), axis=1)\n",
        "y_pred = np.argmax(model.predict(x_test), axis=1)\n",
        "confusion_mtx = tf.math.confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "label_vec = np.arange(num_classes)\n",
        "sns.heatmap(\n",
        "    confusion_mtx,\n",
        "    cmap=\"YlGnBu\",\n",
        "    xticklabels=label_vec,\n",
        "    yticklabels=label_vec,\n",
        "    annot=True,\n",
        "    fmt=\"g\",\n",
        ")\n",
        "plt.xlabel(\"Prediction\")\n",
        "plt.ylabel(\"Label\")\n",
        "cm_name = model_name + \"Confusion_matrix.png\"\n",
        "cm_path = Path(logdir, model_name, cm_name)\n",
        "plt.savefig(cm_path)\n",
        "\n",
        "# save the confusion matrix\n",
        "path_confusion_matrix = Path(logdir, model_name, \"confusion_matrix\")\n",
        "np.save(path_confusion_matrix, confusion_mtx)"
      ],
      "metadata": {
        "id": "_U2rEQkYPQly",
        "outputId": "4ac65389-61dd-417c-8929-0a07e6e33787",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Toshko\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"Toshko\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Toshko\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ Input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m498\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape (\u001b[38;5;33mReshape\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m498\u001b[0m, \u001b[38;5;34m1\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ Input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m498\u001b[0m,    │         \u001b[38;5;34m80\u001b[0m │ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m498\u001b[0m,    │         \u001b[38;5;34m64\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu (\u001b[38;5;33mReLU\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m498\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ re_lu[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │      \u001b[38;5;34m7,696\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │         \u001b[38;5;34m64\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_1 (\u001b[38;5;33mReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │      \u001b[38;5;34m7,696\u001b[0m │ re_lu_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │         \u001b[38;5;34m64\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_2 (\u001b[38;5;33mReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │      \u001b[38;5;34m7,696\u001b[0m │ re_lu_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │         \u001b[38;5;34m64\u001b[0m │ conv2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_3 (\u001b[38;5;33mReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │      \u001b[38;5;34m7,696\u001b[0m │ re_lu_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │         \u001b[38;5;34m64\u001b[0m │ conv2d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ re_lu_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_4 (\u001b[38;5;33mReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │      \u001b[38;5;34m7,696\u001b[0m │ re_lu_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │         \u001b[38;5;34m64\u001b[0m │ conv2d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_5 (\u001b[38;5;33mReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │      \u001b[38;5;34m7,696\u001b[0m │ re_lu_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │         \u001b[38;5;34m64\u001b[0m │ conv2d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_2 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ re_lu_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_6 (\u001b[38;5;33mReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m249\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ re_lu_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │        \u001b[38;5;34m850\u001b[0m │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)        │      \u001b[38;5;34m2,040\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)        │      \u001b[38;5;34m1,230\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │        \u001b[38;5;34m620\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │        \u001b[38;5;34m210\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │         \u001b[38;5;34m22\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ Input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">498</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">498</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ Input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">498</span>,    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">498</span>,    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">498</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ re_lu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,696</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,696</span> │ re_lu_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,696</span> │ re_lu_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ conv2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,696</span> │ re_lu_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ conv2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ re_lu_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,696</span> │ re_lu_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ conv2d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,696</span> │ re_lu_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ conv2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ re_lu_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">249</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ re_lu_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">850</span> │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,040</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,230</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">620</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">210</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m51,676\u001b[0m (201.86 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">51,676</span> (201.86 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m51,452\u001b[0m (200.98 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">51,452</span> (200.98 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m224\u001b[0m (896.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">224</span> (896.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\n",
            "Epoch: 0, accuracy:0.4918,  loss:1.8557,  sparse_categorical_crossentropy:0.7044,  val_accuracy:0.4824,  val_loss:1.6157,  val_sparse_categorical_crossentropy:0.6933,  \n",
            ".55/55 - 27s - 485ms/step - accuracy: 0.4918 - loss: 1.8557 - sparse_categorical_crossentropy: 0.7044 - val_accuracy: 0.4824 - val_loss: 1.6157 - val_sparse_categorical_crossentropy: 0.6933\n",
            "Epoch 2/100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "imZ28QP1PQLQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiKJODFx-0fR"
      },
      "source": [
        "## Understanding the Data\n",
        "\n",
        "The dataset consists of univariate time series, each containing ~2,000-5,000 values with a designated boundary point. For each time series, you need to determine whether a structural break occurred at this boundary point.\n",
        "\n",
        "The data was downloaded when you setup your local environment and is now available in the `data/` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKHXgvjN-0fS",
        "outputId": "387e305a-3eb4-4afd-ea0e-f2775ec61206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\n",
            "data/X_train.parquet: already exists, file length match\n",
            "data/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\n",
            "data/X_test.reduced.parquet: already exists, file length match\n",
            "data/y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\n",
            "data/y_train.parquet: already exists, file length match\n",
            "data/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n",
            "data/y_test.reduced.parquet: already exists, file length match\n"
          ]
        }
      ],
      "source": [
        "# Load the data simply\n",
        "X_train, y_train, X_test = crunch.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T_JmgMq-0fS"
      },
      "source": [
        "### Understanding `X_train`\n",
        "\n",
        "The training data is structured as a pandas DataFrame with a MultiIndex:\n",
        "\n",
        "**Index Levels:**\n",
        "- `id`: Identifies the unique time series\n",
        "- `time`: The timestep within each time series\n",
        "\n",
        "**Columns:**\n",
        "- `value`: The actual time series value at each timestep\n",
        "- `period`: A binary indicator where `0` represents the **period before** the boundary point, and `1` represents the **period after** the boundary point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "0oRCTnOb-0fS",
        "outputId": "2d0663ba-76b2-4937-d7fc-e6c314784242"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>value</th>\n",
              "      <th>period</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th>time</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
              "      <th>0</th>\n",
              "      <td>0.001858</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.001664</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.004386</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000699</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.002433</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">10000</th>\n",
              "      <th>1890</th>\n",
              "      <td>-0.005903</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1891</th>\n",
              "      <td>0.007295</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1892</th>\n",
              "      <td>0.003527</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1893</th>\n",
              "      <td>0.007218</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1894</th>\n",
              "      <td>0.000034</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>23802099 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               value  period\n",
              "id    time                  \n",
              "0     0     0.001858       0\n",
              "      1    -0.001664       0\n",
              "      2    -0.004386       0\n",
              "      3     0.000699       0\n",
              "      4    -0.002433       0\n",
              "...              ...     ...\n",
              "10000 1890 -0.005903       1\n",
              "      1891  0.007295       1\n",
              "      1892  0.003527       1\n",
              "      1893  0.007218       1\n",
              "      1894  0.000034       1\n",
              "\n",
              "[23802099 rows x 2 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WP39dgx-0fS"
      },
      "source": [
        "### Understanding `y_train`\n",
        "\n",
        "This is a simple `pandas.Series` that tells if a dataset id has a structural breakpoint or not.\n",
        "\n",
        "**Index:**\n",
        "- `id`: the ID of the dataset\n",
        "\n",
        "**Value:**\n",
        "- `structural_breakpoint`: Boolean indicating whether a structural break occurred (`True`) or not (`False`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "dPsQPdIj-0fT",
        "outputId": "acd28eab-afd8-44c7-d229-d7e414f2e3c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "id\n",
              "0         True\n",
              "1         True\n",
              "2        False\n",
              "3         True\n",
              "4        False\n",
              "         ...  \n",
              "9996     False\n",
              "9997      True\n",
              "9998     False\n",
              "9999     False\n",
              "10000     True\n",
              "Name: structural_breakpoint, Length: 10001, dtype: bool"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oSS08Ks-0fT"
      },
      "source": [
        "### Understanding `X_test`\n",
        "\n",
        "The test data is provided as a **`list` of `pandas.DataFrame`s** with the same format as [`X_train`](#understanding-X_test).\n",
        "\n",
        "It is structured as a list to encourage processing records one by one, which will be mandatory in the `infer()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ErbKAs--0fT",
        "outputId": "5ce46294-1824-4067-be90-547189e90be3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of datasets: 101\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of datasets:\", len(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "M_dTYXms-0fT",
        "outputId": "b3ee6375-995f-47f6-f6e5-7f08d9838820"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>value</th>\n",
              "      <th>period</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th>time</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"11\" valign=\"top\">10001</th>\n",
              "      <th>0</th>\n",
              "      <td>-0.020657</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.005894</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.003052</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.000590</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.009887</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2517</th>\n",
              "      <td>0.005084</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2518</th>\n",
              "      <td>-0.024414</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2519</th>\n",
              "      <td>-0.014986</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2520</th>\n",
              "      <td>0.012999</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2521</th>\n",
              "      <td>-0.022138</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2522 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               value  period\n",
              "id    time                  \n",
              "10001 0    -0.020657       0\n",
              "      1    -0.005894       0\n",
              "      2    -0.003052       0\n",
              "      3    -0.000590       0\n",
              "      4     0.009887       0\n",
              "...              ...     ...\n",
              "      2517  0.005084       1\n",
              "      2518 -0.024414       1\n",
              "      2519 -0.014986       1\n",
              "      2520  0.012999       1\n",
              "      2521 -0.022138       1\n",
              "\n",
              "[2522 rows x 2 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgulFOGX-0fT"
      },
      "source": [
        "## Strategy Implementation\n",
        "\n",
        "There are multiple approaches you can take to detect structural breaks:\n",
        "\n",
        "1. **Statistical Tests**: Compare distributions before and after the boundary point;\n",
        "2. **Feature Engineering**: Extract features from both segments for comparison;\n",
        "3. **Time Series Modeling**: Detect deviations from expected patterns;\n",
        "4. **Machine Learning**: Train models to recognize break patterns from labeled examples.\n",
        "\n",
        "The baseline implementation below uses a simple statistical approach: a t-test to compare the distributions before and after the boundary point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLfYIXlz-0fT"
      },
      "source": [
        "### The `train()` Function\n",
        "\n",
        "In this function, you build and train your model for making inferences on the test data. Your model must be stored in the `model_directory_path`.\n",
        "\n",
        "The baseline implementation below doesn't require a pre-trained model, as it uses a statistical test that will be computed at inference time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-18T10:04:00.459399Z",
          "start_time": "2024-11-18T10:04:00.455716Z"
        },
        "id": "xQwWDC6M-0fT"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.Series,\n",
        "    model_directory_path: str,\n",
        "):\n",
        "    # For our baseline t-test approach, we don't need to train a model\n",
        "    # This is essentially an unsupervised approach calculated at inference time\n",
        "    model = None\n",
        "\n",
        "    # You could enhance this by training an actual model, for example:\n",
        "    # 1. Extract features from before/after segments of each time series\n",
        "    # 2. Train a classifier using these features and y_train labels\n",
        "    # 3. Save the trained model\n",
        "\n",
        "    joblib.dump(model, os.path.join(model_directory_path, 'model.joblib'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n-jboJH-0fU"
      },
      "source": [
        "### The `infer()` Function\n",
        "\n",
        "In the inference function, your trained model (if any) is loaded and used to make predictions on test data.\n",
        "\n",
        "**Important workflow:**\n",
        "1. Load your model;\n",
        "2. Use the `yield` statement to signal readiness to the runner;\n",
        "3. Process each dataset one by one within the for loop;\n",
        "4. For each dataset, use `yield prediction` to return your prediction.\n",
        "\n",
        "**Note:** The datasets can only be iterated once!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-18T10:03:59.120294Z",
          "start_time": "2024-11-18T10:03:59.114830Z"
        },
        "id": "r1b7hRkl-0fU"
      },
      "outputs": [],
      "source": [
        "def infer(\n",
        "    X_test: typing.Iterable[pd.DataFrame],\n",
        "    model_directory_path: str,\n",
        "):\n",
        "    model = joblib.load(os.path.join(model_directory_path, 'model.joblib'))\n",
        "\n",
        "    yield  # Mark as ready\n",
        "\n",
        "    # X_test can only be iterated once.\n",
        "    # Before getting the next dataset, you must predict the current one.\n",
        "    for dataset in X_test:\n",
        "        # Baseline approach: Compute t-test between values before and after boundary point\n",
        "        # The negative p-value is used as our score - smaller p-values (larger negative numbers)\n",
        "        # indicate more evidence against the null hypothesis that distributions are the same,\n",
        "        # suggesting a structural break\n",
        "        def t_test(u: pd.DataFrame):\n",
        "            return -scipy.stats.ttest_ind(\n",
        "                u[\"value\"][u[\"period\"] == 0],  # Values before boundary point\n",
        "                u[\"value\"][u[\"period\"] == 1],  # Values after boundary point\n",
        "            ).pvalue\n",
        "\n",
        "        prediction = t_test(dataset)\n",
        "        yield prediction  # Send the prediction for the current dataset\n",
        "\n",
        "        # Note: This baseline approach uses a t-test to compare the distributions\n",
        "        # before and after the boundary point. A smaller p-value (larger negative number)\n",
        "        # suggests stronger evidence that the distributions are different,\n",
        "        # indicating a potential structural break."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W0Kl9CA-0fU"
      },
      "source": [
        "## Local testing\n",
        "\n",
        "To make sure your `train()` and `infer()` function are working properly, you can call the `crunch.test()` function that will reproduce the cloud environment locally. <br />\n",
        "Even if it is not perfect, it should give you a quick idea if your model is working properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDZeP-4--0fU"
      },
      "outputs": [],
      "source": [
        "crunch.test(\n",
        "    # Uncomment to disable the train\n",
        "    # force_first_train=False,\n",
        "\n",
        "    # Uncomment to disable the determinism check\n",
        "    # no_determinism_check=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV_5CKs--0fU"
      },
      "source": [
        "## Results\n",
        "\n",
        "Once the local tester is done, you can preview the result stored in `data/prediction.parquet`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ly5q68sA-0fU"
      },
      "outputs": [],
      "source": [
        "prediction = pd.read_parquet(\"data/prediction.parquet\")\n",
        "prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oP-NLGh-0fU"
      },
      "source": [
        "### Local scoring\n",
        "\n",
        "You can call the function that the system uses to estimate your score locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyCrjpzv-0fU"
      },
      "outputs": [],
      "source": [
        "# Load the targets\n",
        "target = pd.read_parquet(\"data/y_test.reduced.parquet\")[\"structural_breakpoint\"]\n",
        "\n",
        "# Call the scoring function\n",
        "sklearn.metrics.roc_auc_score(\n",
        "    target,\n",
        "    prediction,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AE1i3pR-0fV"
      },
      "source": [
        "# Submit your Notebook\n",
        "\n",
        "To submit your work, you must:\n",
        "1. Download your Notebook from Colab\n",
        "2. Upload it to the platform\n",
        "3. Create a run to validate it\n",
        "\n",
        "### >> https://hub.crunchdao.com/competitions/structural-break/submit/notebook\n",
        "\n",
        "![Download and Submit Notebook](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/documentation/animations/download-and-submit-notebook.gif)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}